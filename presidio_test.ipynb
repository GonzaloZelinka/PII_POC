{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTALL DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install presidio_analyzer\n",
    "%pip install presidio_anonymizer\n",
    "%pip install transformers\n",
    "%pip install pandas\n",
    "%pip install spacy\n",
    "%pip install torch\n",
    "%pip install seqeval\n",
    "%pip install spacy-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTALL SIMPLE SPACY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTALL COMPLEX SPACY MODEL (ONLY IF YOU USE THIS INSTEAD OF BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine, RecognizerResult, RecognizerRegistry\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
    "import pandas as pd\n",
    "from transformers_rec import (\n",
    "    TransformersRecognizer,\n",
    "    BERT_DEID_CONFIGURATION,\n",
    ")\n",
    "import logging\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "from typing import List\n",
    "from spacy import displacy\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from seqeval.metrics import accuracy_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import recall_score\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE ANALYZER AND ANONYMIZE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzer_engine(model_path):\n",
    "  \"\"\"Return AnalyzerEngine.\n",
    "    :param model_path: Which model to use for NER:\n",
    "        \"obi/deid_roberta_i2b2\",\n",
    "        \"en_core_web_lg\"\n",
    "    \"\"\"\n",
    "  registry = RecognizerRegistry()\n",
    "  registry.load_predefined_recognizers()\n",
    "  if model_path == \"en_core_web_lg\":\n",
    "    nlp_configuration = {\n",
    "        \"nlp_engine_name\": \"spacy\",\n",
    "        \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"}],\n",
    "    }\n",
    "  elif model_path == \"custom_P_E\":\n",
    "    # nlp = spacy.load(\"models/P&E-model3/model-best\")\n",
    "    # registry.add_recognizer(SpacyRecognizer(nlp))\n",
    "    nlp_configuration = {\n",
    "        \"nlp_engine_name\": \"spacy\",\n",
    "        \"models\": [{\"lang_code\": \"en\", \"model_name\": \"models/P&E-model3/model-best\"}],\n",
    "    }\n",
    "  else:\n",
    "    # Using a small spaCy model + a HF NER model\n",
    "    transformers_recognizer = TransformersRecognizer(model_path=model_path)\n",
    "    if model_path == \"obi/deid_roberta_i2b2\":\n",
    "      transformers_recognizer.load_transformer(**BERT_DEID_CONFIGURATION)\n",
    "    else:\n",
    "      transformers_recognizer.load_transformer(**BERT_PII_CONFIGURATION)\n",
    "    # Use small spaCy model, no need for both spacy and HF models\n",
    "    # The transformers model is used here as a recognizer, not as an NlpEngine\n",
    "    nlp_configuration = {\n",
    "      \"nlp_engine_name\": \"spacy\",\n",
    "      \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_sm\"}],\n",
    "    }\n",
    "    registry.add_recognizer(transformers_recognizer)\n",
    "\n",
    "  nlp_engine = NlpEngineProvider(nlp_configuration=nlp_configuration).create_engine()\n",
    "\n",
    "  analyzer = AnalyzerEngine(nlp_engine=nlp_engine, registry=registry)\n",
    "  return analyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(analyzer, **kwargs):\n",
    "    \"\"\"Analyze input using Analyzer engine and input arguments (kwargs).\"\"\"\n",
    "    if \"entities\" not in kwargs or \"All\" in kwargs[\"entities\"]:\n",
    "        kwargs[\"entities\"] = None\n",
    "    return analyzer.analyze(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymize(text: str, analyze_results: List[RecognizerResult]):\n",
    "    \"\"\"Anonymize identified input using Presidio Anonymizer.\n",
    "    :param text: Full text\n",
    "    :param analyze_results: list of results from presidio analyzer engine\n",
    "    \"\"\"\n",
    "    operator_config = {\"lambda\": lambda x: x}\n",
    "    operator = \"custom\"\n",
    "    res = AnonymizerEngine().anonymize(\n",
    "        text,\n",
    "        analyze_results,\n",
    "        operators={\"DEFAULT\": OperatorConfig(operator, operator_config)},\n",
    "    )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIAL CONFIG FOR THE ANALYZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = analyzer_engine(\"en_core_web_lg\") # \"en_core_web_lg\" or \"obi/deid_roberta_i2b2\" or \"custom_P_E\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.40\n",
    "entities = [\"PERSON\", \"LOCATION\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\",\"CREDIT_CARD\", \"US_SSN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_obj(an_r, text):\n",
    "    \"\"\"Show results of analyze() in a dataframe.\"\"\"\n",
    "    ents = []\n",
    "    for r in an_r:\n",
    "      info = r.to_dict()\n",
    "      ent ={ \"start\": info[\"start\"], \n",
    "              \"end\": info['end'], \n",
    "              \"confidence\": info['score'], \n",
    "              \"entity\": info['entity_type'], \n",
    "              \"text\": text[info[\"start\"]:info[\"end\"]]} \n",
    "      ents.append(ent)\n",
    "    return ents\n",
    "\n",
    "\n",
    "def model_results(csv_path, json_path, entities, threshold, analyzer,columns, check_overlaps=False):\n",
    "  results = []\n",
    "  df = pd.read_csv(csv_path, encoding=\"ISO-8859-1\",header=0, names=columns)\n",
    "  # file = open(csv_path, 'r', encoding=\"ISO-8859-1\")\n",
    "  # reader = csv.reader(file)\n",
    "  # rows = list(reader)\n",
    "  # for row in tqdm(rows, total=len(rows)):\n",
    "  for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    # id = row.PVID\n",
    "    # text = row.CONTENT\n",
    "    text = row[0]\n",
    "    analyze_results = analyze(\n",
    "      analyzer=analyzer,\n",
    "      text=text,\n",
    "      entities= entities,\n",
    "      language=\"en\",\n",
    "      score_threshold=threshold,\n",
    "    )\n",
    "    if check_overlaps: # return only entities without overlaps (resolved from presidio) and prediction.\n",
    "      text_anon = anonymize(text, analyze_results)\n",
    "      text_anon = sorted(text_anon.items, key=lambda x: x.start)\n",
    "      result = []\n",
    "      for i, res in enumerate(text_anon):\n",
    "          result.append({\"start\": res.start, \"end\": res.end, \"entity\": res.entity_type, \"text\": res.text})\n",
    "          \n",
    "    else: # return all entities with overlaps and prediction. \n",
    "      result = create_obj(analyze_results, text)\n",
    "    # results.append({\"PVID\": id, \"TEXT\": text, \"ENTITIES\": result})\n",
    "    results.append({\"TEXT\": text, \"ENTITIES\": result})\n",
    "  fp=open(json_path,'w', encoding=\"ISO-8859-1\") # output file\n",
    "  json.dump(results, fp)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST SIMPLE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(text: str, analyze_results: List[RecognizerResult]):\n",
    "    \"\"\"\n",
    "    Highlights every identified entity on top of the text.\n",
    "    :param text: full text\n",
    "    :param analyze_results: list of analyzer results.\n",
    "    \"\"\"\n",
    "    ents = []\n",
    "\n",
    "    # Use the anonymizer to resolve overlaps\n",
    "    results = anonymize(text, analyze_results)\n",
    "    # sort by start index\n",
    "    results = sorted(results.items, key=lambda x: x.start)\n",
    "    for i, res in enumerate(results):\n",
    "        ents.append({\"start\": res.start, \"end\": res.end, \"label\": res.entity_type, \"text\": res.text})\n",
    "    return [{\"text\": text, \"ents\": ents}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(an_r, text, return_analyzer_results=False):\n",
    "    \"\"\"Show results of analyze() in a dataframe.\"\"\"\n",
    "    df = pd.DataFrame.from_records([r.to_dict() for r in an_r])\n",
    "    df[\"text\"] = [text[res.start: res.end] for res in an_r]\n",
    "    df_subset = df[[\"entity_type\", \"text\", \"start\", \"end\", \"score\"]].rename(\n",
    "        {\n",
    "            \"entity_type\": \"Entity type\",\n",
    "            \"text\": \"Text\",\n",
    "            \"start\": \"Start\",\n",
    "            \"end\": \"End\",\n",
    "            \"score\": \"Confidence\",\n",
    "        },\n",
    "        axis=1,\n",
    "    )\n",
    "    df_subset[\"Text\"] = [text[res.start: res.end] for res in an_r]\n",
    "    #  In analysis_explanation_df there are more columns than in df_subset with more information. \n",
    "    if return_analyzer_results:\n",
    "      analysis_explanation_df = pd.DataFrame.from_records(\n",
    "          [r.analysis_explanation.to_dict() for r in an_r]\n",
    "      )\n",
    "    # df_subset = pd.concat([df_subset, analysis_explanation_df], axis=1)\n",
    "    result = annotate(text, an_r)\n",
    "    return df_subset.reset_index(drop=True), result\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"My name is Gonzalo Zelinka, I'm disappointed with the Sony Xperia.The sound quality seems to be awful. Call me at 212 555 5555 or you can email me at james_kelly@hotmail.it. Disappointing and frustrating. My social SN is 735 92 3223\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_results = analyze(\n",
    "    analyzer=analyzer,\n",
    "    text=text,\n",
    "    entities= entities,\n",
    "    language=\"en\",\n",
    "    score_threshold=threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">My name is Gonzalo Zelinka, I'm disappointed with the Sony Xperia.The sound quality seems to be awful. Call me at \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    212 555 5555\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUMBER</span>\n",
       "</mark>\n",
       " or you can email me at \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    james_kelly@hotmail.it\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EMAIL_ADDRESS</span>\n",
       "</mark>\n",
       ". Disappointing and frustrating. My social SN is \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    735 92 3223\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">US_SSN</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity type</th>\n",
       "      <th>Text</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EMAIL_ADDRESS</td>\n",
       "      <td>james_kelly@hotmail.it</td>\n",
       "      <td>150</td>\n",
       "      <td>172</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US_SSN</td>\n",
       "      <td>735 92 3223</td>\n",
       "      <td>221</td>\n",
       "      <td>232</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PHONE_NUMBER</td>\n",
       "      <td>212 555 5555</td>\n",
       "      <td>114</td>\n",
       "      <td>126</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Entity type                    Text  Start  End  Confidence\n",
       "0  EMAIL_ADDRESS  james_kelly@hotmail.it    150  172         1.0\n",
       "1         US_SSN             735 92 3223    221  232         0.5\n",
       "2   PHONE_NUMBER            212 555 5555    114  126         0.4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frame, sentence = show_results(analyze_results, text)\n",
    "# print(sentence)\n",
    "displacy.render(sentence, style=\"ent\", manual=True)\n",
    "display(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERATE RESULTS FROM COMPLEX DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHANGE DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "#Change this\n",
    "os.chdir(\"\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results(\"testing-data/product_reviews7_test_sentences.csv\", \"testing-data/product_reviews8.json\", entities, threshold, analyzer, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT THE DATA TRUTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span_indx(\n",
    "    labels: List[str],\n",
    "    words: List[str],\n",
    "    sentence: str\n",
    ") -> List[tuple]:\n",
    "    \"\"\"Gets span starts and ends for Spacy spancat component.\n",
    "        \n",
    "        Returns list of tuples where the first element of the \n",
    "        tuple is the span start, the second element of the tuple\n",
    "        is the span end and the third element of the tuple is\n",
    "        the span category. \n",
    "    \"\"\"\n",
    "    #gets list of indices corresponding to labelled words \n",
    "    label_indx = []\n",
    "    temp_list = []\n",
    "\n",
    "    for i, l in enumerate(labels):\n",
    "        if l != 'O':\n",
    "            temp_list.append(i)\n",
    "        else:\n",
    "            label_indx.append(temp_list)\n",
    "            temp_list = []    \n",
    "        if i == len(labels) - 1:\n",
    "            label_indx.append(temp_list)\n",
    "\n",
    "    clean_label_indx = [x for x in label_indx if len(x) > 0]\n",
    "\n",
    "    spans = []\n",
    "    for indx in clean_label_indx:\n",
    "        if len(indx) == 1:\n",
    "            span = words[indx[0]]\n",
    "            label = labels[indx[0]].upper()\n",
    "        else:\n",
    "            span = ' '.join([words[i] for i in indx])  \n",
    "            label = [labels[i].upper() for i in indx][0]\n",
    "        #remove punctuation and strip whitespace for spans\n",
    "        span_clean = span.strip()\n",
    "        for m in re.finditer(re.escape(span_clean), sentence):\n",
    "            spans.append({\"start\":m.start(), \"end\":m.end(), \"entity\": label, \"text\": m.group()})\n",
    "    \n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_csv_annotated_to_json(input_path):\n",
    "    DATA = []\n",
    "    data = (pd.read_csv(input_path, encoding='ISO-8859-1')\n",
    "          .fillna(method='ffill'))\n",
    "    for sent, sent_info in data.groupby('Review #'):\n",
    "      words = list(sent_info[\"Word\"])\n",
    "      #convert words to sentence and get rid of spaces between punctuation characters\n",
    "      sentence = re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', \" \".join(words))\n",
    "      #get labels\n",
    "      labels = list(sent_info['Tag'])\n",
    "      #identify token span start, span ends and span category\n",
    "      span_ents = get_span_indx(labels, words, sentence)\n",
    "      DATA.append({\"TEXT\": sentence, \"ENTITIES\": span_ents})\n",
    "    return DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = transform_csv_annotated_to_json(\"testing-data/product_reviews7_to_evaluate.csv\")\n",
    "with open('testing-data/true_data.json', 'w') as fp:\n",
    "    json.dump(data_csv, fp)\n",
    "print(data_csv[1999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract only sentences to send to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(input_file, output_file):\n",
    "  data = (pd.read_csv(input_file, encoding='ISO-8859-1')\n",
    "    .fillna(method='ffill'))\n",
    "  with open(output_file, 'w', encoding='ISO-8859-1') as fo:\n",
    "    writer = csv.writer(fo)\n",
    "    writer.writerow(['SENTENCES']) \n",
    "    for sent, sent_info in data.groupby('Review #'):\n",
    "      words = list(sent_info[\"Word\"])\n",
    "      sentence = re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', \" \".join(words))\n",
    "      writer.writerow([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_sentences(\"testing-data/product_reviews7_to_evaluate.csv\", \"testing-data/product_reviews7_sentences_to_evaluate.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN THE MODEL WITH THE EXTRACTED SENTENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results(csv_path=\"testing-data/product_reviews7_sentences_to_evaluate.csv\", json_path=\"testing-data/product_reviews7_results_lg.json\", \n",
    "entities=entities, threshold=threshold, analyzer=analyzer, columns=[\"SENTENCES\"], check_overlaps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_array_evaluate(array):\n",
    "  \"\"\"Generate array for evaluation.\"\"\"\n",
    "  array_evaluate = []\n",
    "  text = array['TEXT']\n",
    "  for word in text.split(' '):\n",
    "    is_in = False\n",
    "    for entity in array[\"ENTITIES\"]:\n",
    "      if word in entity[\"text\"]:\n",
    "        is_in = True\n",
    "        if entity[\"entity\"] == \"PHO\":\n",
    "          label = \"PHONE_NUMBER\"\n",
    "        elif entity[\"entity\"] == \"EMAIL\":\n",
    "          label = \"EMAIL_ADDRESS\"\n",
    "        elif entity[\"entity\"] == \"PER\":\n",
    "          label = \"PERSON\"\n",
    "        elif entity[\"entity\"] == \"ORGANIZATION\":\n",
    "          label = 'O'\n",
    "        elif entity[\"entity\"] == \"ADDRESS\":\n",
    "          label = 'LOCATION'\n",
    "        else: \n",
    "          label = entity[\"entity\"]\n",
    "        array_evaluate.append(label)\n",
    "\n",
    "        break\n",
    "    if not is_in:\n",
    "      array_evaluate.append(\"O\")\n",
    "  return array_evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall(ground_truth, predictions, output_file):\n",
    "    \"\"\"Calculate precision and recall.\"\"\"\n",
    "    gt_eval_array = []\n",
    "    pr_eval_array = []\n",
    "    for gt in ground_truth:\n",
    "        a_eval = generate_array_evaluate(gt)\n",
    "        gt_eval_array.append(a_eval)\n",
    "    for pr in predictions:\n",
    "        a_eval = generate_array_evaluate(pr)\n",
    "        pr_eval_array.append(a_eval)\n",
    "    # for i in range(2):\n",
    "    #     print(ground_truth[i])\n",
    "    #     print(gt_eval_array[i])\n",
    "    #     print(pr_eval_array[i])\n",
    "    print(\"General Precision: \", accuracy_score(gt_eval_array, pr_eval_array))\n",
    "    print(\"General Recall: \", recall_score(gt_eval_array, pr_eval_array))\n",
    "    print(\"General F1: \", f1_score(gt_eval_array, pr_eval_array))\n",
    "    \n",
    "    report = classification_report(gt_eval_array, pr_eval_array)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='ISO-8859-1') as f:\n",
    "        f.write(\"General Precision: \" + str(accuracy_score(gt_eval_array, pr_eval_array)) + \"\\n\")\n",
    "        f.write(\"General Recall: \" + str(recall_score(gt_eval_array, pr_eval_array)) + \"\\n\")\n",
    "        f.write(\"General F1: \" + str(f1_score(gt_eval_array, pr_eval_array)) + \"\\n\")\n",
    "        f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', '.* seems not to be NE tag\\.')\n",
    "with open(\"testing-data/output_model_lg.json\", \"r\") as f:\n",
    "  json_data = f.read()\n",
    "prediction_data = json.loads(json_data)\n",
    "calculate_precision_recall(data_csv, prediction_data, \"eval/REPORT_ROBERTA.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a9f895cc7e3ca0d623982e8f7234235464dc9c0b943e7d04631c8d05e4aca2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
