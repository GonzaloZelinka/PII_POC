{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTALL DEPENDENCIES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR MAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR CUDA (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install presidio_analyzer\n",
    "%pip install presidio_anonymizer\n",
    "%pip install transformers\n",
    "%pip install pandas\n",
    "%pip install spacy\n",
    "%pip install spacy-transformers\n",
    "%pip install tabulate\n",
    "%pip install multiprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTALL SIMPLE SPACY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTALL COMPLEX SPACY MODEL (ONLY IF YOU USE THIS INSTEAD OF BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Iterator, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import spacy\n",
    "from presidio_analyzer import RecognizerResult\n",
    "from spacy import displacy\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Config import Config\n",
    "from transformers_rec import (BERT_DEID_CONFIGURATION, Analyzer, Anonymizer,\n",
    "                              TransformersRecognizer)\n",
    "from utils import (fix_entities_to_eval, model_results,\n",
    "                   transform_csv_annotated_to_json)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING CUDA FOR GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIAL CONFIG FOR THE ANALYZER AND MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = Analyzer(\"obi/deid_roberta_i2b2\") # \"en_core_web_lg\" or \"obi/deid_roberta_i2b2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = Config.threshold\n",
    "entities = Config.entities\n",
    "columns = Config.columns\n",
    "number_column_review = Config.number_column_review\n",
    "check_overlaps= Config.check_overlaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST SIMPLE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(text: str, analyze_results: List[RecognizerResult]):\n",
    "    \"\"\"\n",
    "    Highlights every identified entity on top of the text.\n",
    "    :param text: full text\n",
    "    :param analyze_results: list of analyzer results.\n",
    "    \"\"\"\n",
    "    ents = []\n",
    "\n",
    "    # Use the anonymizer to resolve overlaps\n",
    "    anonymizer = Anonymizer()\n",
    "    results = anonymizer.anonymize(text, analyze_results)\n",
    "    # sort by start index\n",
    "    results = sorted(results.items, key=lambda x: x.start)\n",
    "    for i, res in enumerate(results):\n",
    "        ents.append({\"start\": res.start, \"end\": res.end, \"label\": res.entity_type, \"text\": res.text})\n",
    "    return [{\"text\": text, \"ents\": ents}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(an_r, text, return_analyzer_results=False):\n",
    "    \"\"\"Show results of analyze() in a dataframe.\"\"\"\n",
    "    df = pd.DataFrame.from_records([r.to_dict() for r in an_r])\n",
    "    df[\"text\"] = [text[res.start: res.end] for res in an_r]\n",
    "    df_subset = df[[\"entity_type\", \"text\", \"start\", \"end\", \"score\"]].rename(\n",
    "        {\n",
    "            \"entity_type\": \"Entity type\",\n",
    "            \"text\": \"Text\",\n",
    "            \"start\": \"Start\",\n",
    "            \"end\": \"End\",\n",
    "            \"score\": \"Confidence\",\n",
    "        },\n",
    "        axis=1,\n",
    "    )\n",
    "    df_subset[\"Text\"] = [text[res.start: res.end] for res in an_r]\n",
    "    #  In analysis_explanation_df there are more columns than in df_subset with more information. \n",
    "    if return_analyzer_results:\n",
    "      analysis_explanation_df = pd.DataFrame.from_records(\n",
    "          [r.analysis_explanation.to_dict() for r in an_r]\n",
    "      )\n",
    "    # df_subset = pd.concat([df_subset, analysis_explanation_df], axis=1)\n",
    "    result = annotate(text, an_r)\n",
    "    return df_subset.reset_index(drop=True), result\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"This is such a beautiful dress. The material is gorgeous and high quality, and the design is stunning. Fits perfectly, tailored right to my height. Slit is the perfect length. Love the different ways to style it. And no extra customs duty charges importing to Canada! Thanks so much!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_results = analyzer.analyze(\n",
    "    text=text,\n",
    "    entities= entities,\n",
    "    language=\"en\",\n",
    "    score_threshold=threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame, sentence = show_results(analyze_results, text)\n",
    "# print(sentence)\n",
    "displacy.render(sentence, style=\"ent\", manual=True)\n",
    "display(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACT SENTENCES TO SEND THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_to_eval(input_file, output_file):\n",
    "  data = (pd.read_csv(input_file, encoding='ISO-8859-1')\n",
    "    .fillna(method='ffill'))\n",
    "  with open(output_file, 'w', encoding='ISO-8859-1') as fo:\n",
    "    writer = csv.writer(fo)\n",
    "    num_reviews = 0\n",
    "    for index, row in data.iterrows():\n",
    "      reviews_list = eval(row['REVIEWS'])\n",
    "      for review in reviews_list:\n",
    "        num_reviews = num_reviews + 1\n",
    "        new_row = [str(row[\"SITE_URL\"]), row[\"ITEM_GROUP_ID\"], row[\"PVIDS\"], str(review)]\n",
    "        # concadenated_columns = '[,]'.join(new_row)\n",
    "        writer.writerow(new_row)\n",
    "    return num_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reviews = extract_sentences_to_eval(\"testing-data/Copy_of_(#65766)_Flamingo_Reviews_Extracted_Data_2023_04_13 (2).csv\", \"testing-data/cleaned_sentences_from_db2.csv\")\n",
    "print(f\"Number of reviews: {num_reviews}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gonzalo.zelinka/Desktop/PII_POC/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting results for the ner model and presidio. Progressively writing the results in testing-data/output_from_db3.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/130639 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://iedm.com\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 't' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (fix_entities_to_eval, model_results,\n\u001b[1;32m      2\u001b[0m                    transform_csv_annotated_to_json)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtesting-data/cleaned_sentences_from_db2.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtesting-data/output_from_db3.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PII_POC/utils/run_model.py:65\u001b[0m, in \u001b[0;36mmodel_results\u001b[0;34m(input_path, output_path)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     result \u001b[39m=\u001b[39m create_obj(result, row[\u001b[39m3\u001b[39m])\n\u001b[0;32m---> 65\u001b[0m row \u001b[39m=\u001b[39m [row[\u001b[39m0\u001b[39m], row[\u001b[39m1\u001b[39m], \u001b[39meval\u001b[39;49m(row[\u001b[39m2\u001b[39;49m]), row[\u001b[39m3\u001b[39m], json\u001b[39m.\u001b[39mdumps(result)]\n\u001b[1;32m     66\u001b[0m \u001b[39m# final_result = {\"SITE_URL\": new_text[0], \"ITEM_GROUP_ID\":new_text[1], \"PVID\": eval(new_text[2]), \"TEXT\": text[3], \"ENTITIES\": result}\u001b[39;00m\n\u001b[1;32m     67\u001b[0m writer\u001b[39m.\u001b[39mwriterow(row)\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 't' is not defined"
     ]
    }
   ],
   "source": [
    "from utils import (fix_entities_to_eval, model_results,\n",
    "                   transform_csv_annotated_to_json)\n",
    "model_results(input_path=\"testing-data/cleaned_sentences_from_db2.csv\", output_path=\"testing-data/output_from_db3.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATE MODEL\n",
    "The format to the dataset if like this: \n",
    "- [Example Dataset](https://www.kaggle.com/datasets/namanj27/ner-dataset)\n",
    "\n",
    "Column necessaries and their names:\n",
    "- Sentence # --> Review #\n",
    "- Word --> Word\n",
    "- POS --> Delete this.\n",
    "- Tag --> Tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT THE GROUND TRUTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = transform_csv_annotated_to_json(\"testing-data/product_reviews9.csv\")\n",
    "print(ground_truth[1999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT ONLY SENTENCES TO SEND TO THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_to_eval(input_file, output_file):\n",
    "  data = (pd.read_csv(input_file, encoding='ISO-8859-1')\n",
    "    .fillna(method='ffill'))\n",
    "  with open(output_file, 'w', encoding='ISO-8859-1') as fo:\n",
    "    writer = csv.writer(fo)\n",
    "    writer.writerow(['SENTENCES']) \n",
    "    for sent, sent_info in data.groupby('Review #'):\n",
    "      words = list(sent_info[\"Word\"])\n",
    "      sentence = re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', \" \".join(words))\n",
    "      writer.writerow([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_sentences_to_eval(\"testing-data/product_reviews9.csv\", \"testing-data/sentences_evaluate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_from_db(input_file, output_file):\n",
    "  data = (pd.read_csv(input_file, encoding='ISO-8859-1')\n",
    "    .fillna(method='ffill'))\n",
    "  reviews = []\n",
    "  for idx, row in tqdm(data.iterrows(), total=len(data)):\n",
    "    for review in json.loads(row[\"REVIEWS\"]):\n",
    "      reviews.append([row[\"SITE_URL\"], row[\"PVID\"], review])\n",
    "  new_data = pd.DataFrame(reviews, columns=[\"SITE_URL\", \"PVID\", \"REVIEW\"])\n",
    "  new_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_sentences_from_db(\"testing-data/reviews.csv\", \"testing-data/sentences_from_db2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN THE MODEL WITH THE EXTRACTED SENTENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results(input_path=\"testing-data/input-path.csv\", output_path=\"testing-data/output-path.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REMOVE UNWANTED ENTITIES FROM THE MODEL RESULTS AND JOIN ENTITIES SUBDIVIDED INTO ONE\n",
    "- Sometimes the model divide entities into several entities, so we need to join them to evaluate the results. For example, if the model found the entity \"686 E Broadway\" as \"6\", \"86 E\" and \"Broadway\", we need to join them to evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_entities_to_eval(\"testing-data/input_path.json\", \"testing-data/output_path.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testing-data/output_path.json\", \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "  json_data = f.read()\n",
    "prediction_data = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_general_scores(entity_scores):\n",
    "    precision_sum = 0\n",
    "    recall_sum = 0\n",
    "    f1_sum = 0\n",
    "    for entity, scores in entity_scores.items():\n",
    "        precision_sum += scores['precision']\n",
    "        recall_sum += scores['recall']\n",
    "        f1_sum += scores['f1']\n",
    "\n",
    "    num_entities = len(entity_scores)\n",
    "    general_precision = precision_sum / num_entities\n",
    "    general_recall = recall_sum / num_entities\n",
    "    general_f1 = f1_sum / num_entities\n",
    "\n",
    "    return general_precision, general_recall, general_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE CASE: EXACT\n",
    "- Identifies the exact words associated with all PII entities in the input text.\n",
    "- This use case is applicable if the client wants to know which exact words correspond to the PII information. For example to apply masks over the PII entities detected in the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall_for_entities(ground_truth, output_model, unique_entities, output_file):\n",
    "    # Initialize counters for true positives, false positives, and false negatives\n",
    "    tp = {entity: 0 for entity in unique_entities}\n",
    "    fp = {entity: 0 for entity in unique_entities}\n",
    "    fn = {entity: 0 for entity in unique_entities}\n",
    "\n",
    "    for example_idx in range(len(ground_truth)):\n",
    "        ground_truth_entities = ground_truth[example_idx]['ENTITIES']\n",
    "        output_entities = output_model[example_idx]['ENTITIES']\n",
    "        # create a set of output entity texts for quick lookup\n",
    "        ground_truth_texts = set([ent['text'].replace(\" \", \"\") for ent in ground_truth_entities])\n",
    "\n",
    "        for ground_truth_ent in ground_truth_entities:\n",
    "            ent_type = ground_truth_ent['entity']\n",
    "            if ent_type not in unique_entities:\n",
    "                continue\n",
    "            # print(output_entities)\n",
    "            if any([ent['text'].replace(\" \", \"\") == ground_truth_ent['text'].replace(\" \",\"\") and ent[\"start\"] == ground_truth_ent[\"start\"] for ent in output_entities]):\n",
    "                tp[ent_type] += 1\n",
    "            else:\n",
    "                fn[ent_type] += 1\n",
    "        for output_ent in output_entities:\n",
    "            ent_type = output_ent['entity']\n",
    "            if ent_type not in unique_entities:\n",
    "                continue\n",
    "            if output_ent['text'].replace(\" \",\"\") not in ground_truth_texts:\n",
    "                fp[ent_type] += 1\n",
    "    # Calculate precision, recall, and F1 score for each entity\n",
    "    scores = {}\n",
    "    table = []\n",
    "    headers = ['Entity', 'Precision', 'Recall', 'F1']\n",
    "    for entity in unique_entities:\n",
    "        p = round(tp[entity] / (tp[entity] + fp[entity]), 2) if tp[entity] + fp[entity] > 0 else 0\n",
    "        r = round(tp[entity] / (tp[entity] + fn[entity]), 2) if tp[entity] + fn[entity] > 0 else 0\n",
    "        f1 = round(2 * p * r / (p + r), 2) if p + r > 0 else 0\n",
    "        scores[entity] = {'precision': p, 'recall': r, 'f1': f1}\n",
    "        table.append([entity, p, r, f1])\n",
    "    \n",
    "    general_precision, general_recall, general_f1 = calculate_general_scores(scores)\n",
    "    report = f\"\\nGeneral Precision: {general_precision}\\nGeneral Recall: {general_recall}\\nGeneral F1: {general_f1}\\n{tabulate(table, headers)}\"\n",
    "    with open(output_file, 'w', encoding='ISO-8859-1') as f:\n",
    "        f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_precision_recall_for_entities(ground_truth, prediction_data,set([\"PERSON\", \"EMAIL_ADDRESS\", \"LOCATION\", \"US_SSN\", \"CREDIT_CARD\", \"PHONE_NUMBER\"]) , \"eval/REPORT_LG2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE CASE: BINARY\n",
    "- Given an input text, it indicates whether each entity (person, location, credit_card, phone_number, us_ssn) is present at least once or not.\n",
    "- The use case is applicable for filtering out reviews with sensitive information without needing to know which part of the text has the sensitive information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_entities(ground_truth, output_model, unique_entities, output_file):\n",
    "    # Initialize counters for true positives, false positives, and false negatives\n",
    "    tp = {entity: 0 for entity in unique_entities}\n",
    "    fp = {entity: 0 for entity in unique_entities}\n",
    "    fn = {entity: 0 for entity in unique_entities}\n",
    "    \n",
    "    # Loop over the documents in the output model\n",
    "    for i, doc in enumerate(output_model):\n",
    "        # Get the set of entities present in the document\n",
    "        model_entity_set = set()\n",
    "        for entity in doc['ENTITIES']:\n",
    "            model_entity_set.add(entity['entity'])\n",
    "        # Get the set of entities present in the ground truth in the same index of output model\n",
    "        ground_truth_set = set()\n",
    "        for entity in ground_truth[i]['ENTITIES']:\n",
    "            ground_truth_set.add(entity['entity'])\n",
    "            \n",
    "        # Check if each entity in the ground truth is present in the output model\n",
    "        for entity in ground_truth_set:\n",
    "            if entity in model_entity_set:\n",
    "                # Entity is present in both ground truth and output model\n",
    "                tp[entity] += 1\n",
    "            else:\n",
    "                # Entity is present in ground truth but not in output model\n",
    "                fn[entity] += 1\n",
    "        \n",
    "        # Check if each entity in the output model is a false positive\n",
    "        for entity in model_entity_set:\n",
    "            if entity not in ground_truth_set:\n",
    "                # Entity is not in the ground truth\n",
    "                fp[entity] += 1\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score for each entity\n",
    "    scores = {}\n",
    "    table = []\n",
    "    headers = ['Entity', 'Precision', 'Recall', 'F1']\n",
    "    for entity in unique_entities:\n",
    "        p = round(tp[entity] / (tp[entity] + fp[entity]), 2) if tp[entity] + fp[entity] > 0 else 0\n",
    "        r = round(tp[entity] / (tp[entity] + fn[entity]), 2) if tp[entity] + fn[entity] > 0 else 0\n",
    "        f1 = round(2 * p * r / (p + r), 2) if p + r > 0 else 0\n",
    "        scores[entity] = {'precision': p, 'recall': r, 'f1': f1}\n",
    "        table.append([entity, p, r, f1])\n",
    "    \n",
    "    general_precision, general_recall, general_f1 = calculate_general_scores(scores)\n",
    "    report = f\"\\nGeneral Precision: {general_precision}\\nGeneral Recall: {general_recall}\\nGeneral F1: {general_f1}\\n{tabulate(table, headers)}\"\n",
    "    with open(output_file, 'w', encoding='ISO-8859-1') as f:\n",
    "        f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1 = [{'TEXT': 'a1', \"ENTITIES\": [{'start': 21, 'end': 33, 'entity': 'PERSON', 'text': 'Zoey Edwards'}, {'start': 129, 'end': 151, 'entity': 'EMAIL_ADDRESS', 'text': 'edwards-zoey@gmail.com'}, {'start': 190, 'end': 201, 'entity': 'LOCATION', 'text': '900 F St NW'}, {'start': 213, 'end': 222, 'entity': 'US_SSN', 'text': '367245504'}, {'start': 252, 'end': 271, 'entity': 'CREDIT_CARD', 'text': '2259-8740-7030-1462'}]}]\n",
    "# a2 = [{\"TEXT\": \"a2\", \"ENTITIES\": [{'start': 21, 'end': 33, 'entity': 'PERSON', 'text': 'Zoey Edwards'}, {'start': 129, 'end': 151, 'entity': 'EMAIL_ADDRESS', 'text': 'edwards-zoey@gmail.com'}, {'start': 190, 'end': 195, 'entity': 'LOCATION', 'text': '900 F'}, {'start': 196, 'end': 201, 'entity': 'LOCATION', 'text': 'St NW'}, {'start': 213, 'end': 222, 'entity': 'US_SSN', 'text': '367245504'}, {'start': 252, 'end': 255, 'entity': 'PHONE_NUMBER', 'text': '225'}, {'start': 255, 'end': 271, 'entity': 'PHONE_NUMBER', 'text': '9-8740-7030-1462'}]}]\n",
    "evaluate_entities(ground_truth, prediction_data, set([\"PERSON\", \"EMAIL_ADDRESS\", \"LOCATION\", \"US_SSN\", \"CREDIT_CARD\", \"PHONE_NUMBER\"]), \"eval/REPORT_SENTENCE_LG.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a9f895cc7e3ca0d623982e8f7234235464dc9c0b943e7d04631c8d05e4aca2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
