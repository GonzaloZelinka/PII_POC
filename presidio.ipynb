{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTALL DEPENDENCIES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR MAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR CUDA (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install presidio_analyzer\n",
    "%pip install presidio_anonymizer\n",
    "%pip install transformers\n",
    "%pip install pandas\n",
    "%pip install spacy\n",
    "%pip install spacy-transformers\n",
    "%pip install tabulate\n",
    "%pip install multiprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTALL SIMPLE SPACY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTALL COMPLEX SPACY MODEL (ONLY IF YOU USE THIS INSTEAD OF BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers_rec import (\n",
    "    TransformersRecognizer,\n",
    "    BERT_DEID_CONFIGURATION,\n",
    "    Analyzer,\n",
    "    Anonymizer,\n",
    ")\n",
    "from typing import List, Iterator, Tuple\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import warnings\n",
    "from tabulate import tabulate\n",
    "import os \n",
    "import time\n",
    "from datetime import datetime\n",
    "import psutil\n",
    "from Config import Config\n",
    "from utils import (\n",
    "    transform_csv_annotated_to_json,\n",
    "    model_results,\n",
    "    fix_entities_to_eval,\n",
    ")\n",
    "from presidio_analyzer import RecognizerResult"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING CUDA FOR GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIAL CONFIG FOR THE ANALYZER AND MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = Analyzer(\"obi/deid_roberta_i2b2\") # \"en_core_web_lg\" or \"obi/deid_roberta_i2b2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = Config.threshold\n",
    "entities = Config.entities\n",
    "columns = Config.columns\n",
    "number_column_review = Config.number_column_review\n",
    "check_overlaps= Config.check_overlaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST SIMPLE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(text: str, analyze_results: List[RecognizerResult]):\n",
    "    \"\"\"\n",
    "    Highlights every identified entity on top of the text.\n",
    "    :param text: full text\n",
    "    :param analyze_results: list of analyzer results.\n",
    "    \"\"\"\n",
    "    ents = []\n",
    "\n",
    "    # Use the anonymizer to resolve overlaps\n",
    "    anonymizer = Anonymizer()\n",
    "    results = anonymizer.anonymize(text, analyze_results)\n",
    "    # sort by start index\n",
    "    results = sorted(results.items, key=lambda x: x.start)\n",
    "    for i, res in enumerate(results):\n",
    "        ents.append({\"start\": res.start, \"end\": res.end, \"label\": res.entity_type, \"text\": res.text})\n",
    "    return [{\"text\": text, \"ents\": ents}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(an_r, text, return_analyzer_results=False):\n",
    "    \"\"\"Show results of analyze() in a dataframe.\"\"\"\n",
    "    df = pd.DataFrame.from_records([r.to_dict() for r in an_r])\n",
    "    df[\"text\"] = [text[res.start: res.end] for res in an_r]\n",
    "    df_subset = df[[\"entity_type\", \"text\", \"start\", \"end\", \"score\"]].rename(\n",
    "        {\n",
    "            \"entity_type\": \"Entity type\",\n",
    "            \"text\": \"Text\",\n",
    "            \"start\": \"Start\",\n",
    "            \"end\": \"End\",\n",
    "            \"score\": \"Confidence\",\n",
    "        },\n",
    "        axis=1,\n",
    "    )\n",
    "    df_subset[\"Text\"] = [text[res.start: res.end] for res in an_r]\n",
    "    #  In analysis_explanation_df there are more columns than in df_subset with more information. \n",
    "    if return_analyzer_results:\n",
    "      analysis_explanation_df = pd.DataFrame.from_records(\n",
    "          [r.analysis_explanation.to_dict() for r in an_r]\n",
    "      )\n",
    "    # df_subset = pd.concat([df_subset, analysis_explanation_df], axis=1)\n",
    "    result = annotate(text, an_r)\n",
    "    return df_subset.reset_index(drop=True), result\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"I have ordered 3 beautiful garments from this shop and have nothing but great things to say about the products, the customer service and the shipping. Diarrablu is definitely my new favorite brand. Love from the Netherlands, Tasha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_results = analyzer.analyze(\n",
    "    text=text,\n",
    "    entities= entities,\n",
    "    language=\"en\",\n",
    "    score_threshold=threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame, sentence = show_results(analyze_results, text)\n",
    "# print(sentence)\n",
    "displacy.render(sentence, style=\"ent\", manual=True)\n",
    "display(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path, output_path, entities, threshold, analyzer, columns, number_column_review, check_overlaps=False\n",
    "model_results(input_path=\"testing-data/input-path.csv\", output_path=\"testing-data/output-path.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATE MODEL\n",
    "The format to the dataset if like this: \n",
    "- [Example Dataset](https://www.kaggle.com/datasets/namanj27/ner-dataset)\n",
    "\n",
    "Column necessaries and their names:\n",
    "- Sentence # --> Review #\n",
    "- Word --> Word\n",
    "- POS --> Delete this.\n",
    "- Tag --> Tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT THE GROUND TRUTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = transform_csv_annotated_to_json(\"testing-data/product_reviews9.csv\")\n",
    "print(ground_truth[1999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT ONLY SENTENCES TO SEND TO THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_to_eval(input_file, output_file):\n",
    "  data = (pd.read_csv(input_file, encoding='ISO-8859-1')\n",
    "    .fillna(method='ffill'))\n",
    "  with open(output_file, 'w', encoding='ISO-8859-1') as fo:\n",
    "    writer = csv.writer(fo)\n",
    "    writer.writerow(['SENTENCES']) \n",
    "    for sent, sent_info in data.groupby('Review #'):\n",
    "      words = list(sent_info[\"Word\"])\n",
    "      sentence = re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', \" \".join(words))\n",
    "      writer.writerow([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_sentences_to_eval(\"testing-data/product_reviews9.csv\", \"testing-data/sentences_evaluate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_from_db(input_file, output_file):\n",
    "  data = (pd.read_csv(input_file, encoding='ISO-8859-1')\n",
    "    .fillna(method='ffill'))\n",
    "  reviews = []\n",
    "  for idx, row in tqdm(data.iterrows(), total=len(data)):\n",
    "    for review in json.loads(row[\"REVIEWS\"]):\n",
    "      reviews.append([row[\"SITE_URL\"], row[\"PVID\"], review])\n",
    "  new_data = pd.DataFrame(reviews, columns=[\"SITE_URL\", \"PVID\", \"REVIEW\"])\n",
    "  new_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68049/68049 [00:07<00:00, 9409.85it/s] \n"
     ]
    }
   ],
   "source": [
    "extract_sentences_from_db(\"testing-data/reviews.csv\", \"testing-data/sentences_from_db2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN THE MODEL WITH THE EXTRACTED SENTENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results(input_path=\"testing-data/input-path.csv\", output_path=\"testing-data/output-path.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REMOVE UNWANTED ENTITIES FROM THE MODEL RESULTS AND JOIN ENTITIES SUBDIVIDED INTO ONE\n",
    "- Sometimes the model divide entities into several entities, so we need to join them to evaluate the results. For example, if the model found the entity \"686 E Broadway\" as \"6\", \"86 E\" and \"Broadway\", we need to join them to evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_entities_to_eval(\"testing-data/input_path.json\", \"testing-data/output_path.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testing-data/output_path.json\", \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "  json_data = f.read()\n",
    "prediction_data = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_general_scores(entity_scores):\n",
    "    precision_sum = 0\n",
    "    recall_sum = 0\n",
    "    f1_sum = 0\n",
    "    for entity, scores in entity_scores.items():\n",
    "        precision_sum += scores['precision']\n",
    "        recall_sum += scores['recall']\n",
    "        f1_sum += scores['f1']\n",
    "\n",
    "    num_entities = len(entity_scores)\n",
    "    general_precision = precision_sum / num_entities\n",
    "    general_recall = recall_sum / num_entities\n",
    "    general_f1 = f1_sum / num_entities\n",
    "\n",
    "    return general_precision, general_recall, general_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE CASE: EXACT\n",
    "- Identifies the exact words associated with all PII entities in the input text.\n",
    "- This use case is applicable if the client wants to know which exact words correspond to the PII information. For example to apply masks over the PII entities detected in the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall_for_entities(ground_truth, output_model, unique_entities, output_file):\n",
    "    # Initialize counters for true positives, false positives, and false negatives\n",
    "    tp = {entity: 0 for entity in unique_entities}\n",
    "    fp = {entity: 0 for entity in unique_entities}\n",
    "    fn = {entity: 0 for entity in unique_entities}\n",
    "\n",
    "    for example_idx in range(len(ground_truth)):\n",
    "        ground_truth_entities = ground_truth[example_idx]['ENTITIES']\n",
    "        output_entities = output_model[example_idx]['ENTITIES']\n",
    "        # create a set of output entity texts for quick lookup\n",
    "        ground_truth_texts = set([ent['text'].replace(\" \", \"\") for ent in ground_truth_entities])\n",
    "\n",
    "        for ground_truth_ent in ground_truth_entities:\n",
    "            ent_type = ground_truth_ent['entity']\n",
    "            if ent_type not in unique_entities:\n",
    "                continue\n",
    "            # print(output_entities)\n",
    "            if any([ent['text'].replace(\" \", \"\") == ground_truth_ent['text'].replace(\" \",\"\") and ent[\"start\"] == ground_truth_ent[\"start\"] for ent in output_entities]):\n",
    "                tp[ent_type] += 1\n",
    "            else:\n",
    "                fn[ent_type] += 1\n",
    "        for output_ent in output_entities:\n",
    "            ent_type = output_ent['entity']\n",
    "            if ent_type not in unique_entities:\n",
    "                continue\n",
    "            if output_ent['text'].replace(\" \",\"\") not in ground_truth_texts:\n",
    "                fp[ent_type] += 1\n",
    "    # Calculate precision, recall, and F1 score for each entity\n",
    "    scores = {}\n",
    "    table = []\n",
    "    headers = ['Entity', 'Precision', 'Recall', 'F1']\n",
    "    for entity in unique_entities:\n",
    "        p = round(tp[entity] / (tp[entity] + fp[entity]), 2) if tp[entity] + fp[entity] > 0 else 0\n",
    "        r = round(tp[entity] / (tp[entity] + fn[entity]), 2) if tp[entity] + fn[entity] > 0 else 0\n",
    "        f1 = round(2 * p * r / (p + r), 2) if p + r > 0 else 0\n",
    "        scores[entity] = {'precision': p, 'recall': r, 'f1': f1}\n",
    "        table.append([entity, p, r, f1])\n",
    "    \n",
    "    general_precision, general_recall, general_f1 = calculate_general_scores(scores)\n",
    "    report = f\"\\nGeneral Precision: {general_precision}\\nGeneral Recall: {general_recall}\\nGeneral F1: {general_f1}\\n{tabulate(table, headers)}\"\n",
    "    with open(output_file, 'w', encoding='ISO-8859-1') as f:\n",
    "        f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_precision_recall_for_entities(ground_truth, prediction_data,set([\"PERSON\", \"EMAIL_ADDRESS\", \"LOCATION\", \"US_SSN\", \"CREDIT_CARD\", \"PHONE_NUMBER\"]) , \"eval/REPORT_LG2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE CASE: BINARY\n",
    "- Given an input text, it indicates whether each entity (person, location, credit_card, phone_number, us_ssn) is present at least once or not.\n",
    "- The use case is applicable for filtering out reviews with sensitive information without needing to know which part of the text has the sensitive information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_entities(ground_truth, output_model, unique_entities, output_file):\n",
    "    # Initialize counters for true positives, false positives, and false negatives\n",
    "    tp = {entity: 0 for entity in unique_entities}\n",
    "    fp = {entity: 0 for entity in unique_entities}\n",
    "    fn = {entity: 0 for entity in unique_entities}\n",
    "    \n",
    "    # Loop over the documents in the output model\n",
    "    for i, doc in enumerate(output_model):\n",
    "        # Get the set of entities present in the document\n",
    "        model_entity_set = set()\n",
    "        for entity in doc['ENTITIES']:\n",
    "            model_entity_set.add(entity['entity'])\n",
    "        # Get the set of entities present in the ground truth in the same index of output model\n",
    "        ground_truth_set = set()\n",
    "        for entity in ground_truth[i]['ENTITIES']:\n",
    "            ground_truth_set.add(entity['entity'])\n",
    "            \n",
    "        # Check if each entity in the ground truth is present in the output model\n",
    "        for entity in ground_truth_set:\n",
    "            if entity in model_entity_set:\n",
    "                # Entity is present in both ground truth and output model\n",
    "                tp[entity] += 1\n",
    "            else:\n",
    "                # Entity is present in ground truth but not in output model\n",
    "                fn[entity] += 1\n",
    "        \n",
    "        # Check if each entity in the output model is a false positive\n",
    "        for entity in model_entity_set:\n",
    "            if entity not in ground_truth_set:\n",
    "                # Entity is not in the ground truth\n",
    "                fp[entity] += 1\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score for each entity\n",
    "    scores = {}\n",
    "    table = []\n",
    "    headers = ['Entity', 'Precision', 'Recall', 'F1']\n",
    "    for entity in unique_entities:\n",
    "        p = round(tp[entity] / (tp[entity] + fp[entity]), 2) if tp[entity] + fp[entity] > 0 else 0\n",
    "        r = round(tp[entity] / (tp[entity] + fn[entity]), 2) if tp[entity] + fn[entity] > 0 else 0\n",
    "        f1 = round(2 * p * r / (p + r), 2) if p + r > 0 else 0\n",
    "        scores[entity] = {'precision': p, 'recall': r, 'f1': f1}\n",
    "        table.append([entity, p, r, f1])\n",
    "    \n",
    "    general_precision, general_recall, general_f1 = calculate_general_scores(scores)\n",
    "    report = f\"\\nGeneral Precision: {general_precision}\\nGeneral Recall: {general_recall}\\nGeneral F1: {general_f1}\\n{tabulate(table, headers)}\"\n",
    "    with open(output_file, 'w', encoding='ISO-8859-1') as f:\n",
    "        f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1 = [{'TEXT': 'a1', \"ENTITIES\": [{'start': 21, 'end': 33, 'entity': 'PERSON', 'text': 'Zoey Edwards'}, {'start': 129, 'end': 151, 'entity': 'EMAIL_ADDRESS', 'text': 'edwards-zoey@gmail.com'}, {'start': 190, 'end': 201, 'entity': 'LOCATION', 'text': '900 F St NW'}, {'start': 213, 'end': 222, 'entity': 'US_SSN', 'text': '367245504'}, {'start': 252, 'end': 271, 'entity': 'CREDIT_CARD', 'text': '2259-8740-7030-1462'}]}]\n",
    "# a2 = [{\"TEXT\": \"a2\", \"ENTITIES\": [{'start': 21, 'end': 33, 'entity': 'PERSON', 'text': 'Zoey Edwards'}, {'start': 129, 'end': 151, 'entity': 'EMAIL_ADDRESS', 'text': 'edwards-zoey@gmail.com'}, {'start': 190, 'end': 195, 'entity': 'LOCATION', 'text': '900 F'}, {'start': 196, 'end': 201, 'entity': 'LOCATION', 'text': 'St NW'}, {'start': 213, 'end': 222, 'entity': 'US_SSN', 'text': '367245504'}, {'start': 252, 'end': 255, 'entity': 'PHONE_NUMBER', 'text': '225'}, {'start': 255, 'end': 271, 'entity': 'PHONE_NUMBER', 'text': '9-8740-7030-1462'}]}]\n",
    "evaluate_entities(ground_truth, prediction_data, set([\"PERSON\", \"EMAIL_ADDRESS\", \"LOCATION\", \"US_SSN\", \"CREDIT_CARD\", \"PHONE_NUMBER\"]), \"eval/REPORT_SENTENCE_LG.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a9f895cc7e3ca0d623982e8f7234235464dc9c0b943e7d04631c8d05e4aca2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
