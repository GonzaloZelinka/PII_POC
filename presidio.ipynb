{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTALL DEPENDENCIES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR MAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR CUDA (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: presidio_analyzer in ./.venv/lib/python3.9/site-packages (2.2.32)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.9/site-packages (from presidio_analyzer) (6.0)\n",
      "Requirement already satisfied: phonenumbers>=8.12 in ./.venv/lib/python3.9/site-packages (from presidio_analyzer) (8.13.8)\n",
      "Requirement already satisfied: regex in ./.venv/lib/python3.9/site-packages (from presidio_analyzer) (2023.3.23)\n",
      "Requirement already satisfied: spacy>=3.4.4 in ./.venv/lib/python3.9/site-packages (from presidio_analyzer) (3.5.1)\n",
      "Requirement already satisfied: tldextract in ./.venv/lib/python3.9/site-packages (from presidio_analyzer) (3.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from spacy>=3.4.4->presidio_analyzer) (23.0)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from spacy>=3.4.4->presidio_analyzer) (3.1.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.venv/lib/python3.9/site-packages (from spacy>=3.4.4->presidio_analyzer) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.9/site-packages (from spacy>=3.4.4->presidio_analyzer) (2.28.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.9/site-packages (from spacy>=3.4.4->presidio_analyzer) (67.6.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./.venv/lib/python3.9/site-packages (from spacy>=3.4.4->presidio_analyzer) (0.10.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.9/site-packages (from spacy>=3.4.4->presidio_analyzer) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./.venv/lib/python3.9/site-packages (from spacy>=3.4.4->presidio_analyzer) (1.24.2)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in ./.venv/lib/python3.9/site-packages (from spacy>=3.4.4->presidio_analyzer) (0.7.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./.venv/lib/python3.9/site-packages (from spacy>=3.4.4->presidio_analyzer) (6.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.9/site-packages (from spacy>=3.4.4->presidio_analyzer) (2.4.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.9/site-packages (from spacy>=3.4.4->presidio_analyzer) (2.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./.venv/lib/python3.9/site-packages (from spacy>=3.4.4->presidio_analyzer) (1.10.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.9/site-packages (from spacy>=3.4.4->presidio_analyzer) (3.0.12)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.9/site-packages (from spacy>=3.4.4->presidio_analyzer) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in ./.venv/lib/python3.9/site-packages (from spacy>=3.4.4->presidio_analyzer) (8.1.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.9/site-packages (from spacy>=3.4.4->presidio_analyzer) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.9/site-packages (from spacy>=3.4.4->presidio_analyzer) (1.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.9/site-packages (from spacy>=3.4.4->presidio_analyzer) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.9/site-packages (from spacy>=3.4.4->presidio_analyzer) (1.1.1)\n",
      "Requirement already satisfied: requests-file>=1.4 in ./.venv/lib/python3.9/site-packages (from tldextract->presidio_analyzer) (1.5.1)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.9/site-packages (from tldextract->presidio_analyzer) (3.4)\n",
      "Requirement already satisfied: filelock>=3.0.8 in ./.venv/lib/python3.9/site-packages (from tldextract->presidio_analyzer) (3.10.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./.venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy>=3.4.4->presidio_analyzer) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.4.4->presidio_analyzer) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.4.4->presidio_analyzer) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.4.4->presidio_analyzer) (1.26.15)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.9/site-packages (from requests-file>=1.4->tldextract->presidio_analyzer) (1.16.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./.venv/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.4.4->presidio_analyzer) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.4.4->presidio_analyzer) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./.venv/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy>=3.4.4->presidio_analyzer) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->spacy>=3.4.4->presidio_analyzer) (2.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: presidio_anonymizer in ./.venv/lib/python3.9/site-packages (2.2.32)\n",
      "Requirement already satisfied: pycryptodome>=3.10.1 in ./.venv/lib/python3.9/site-packages (from presidio_anonymizer) (3.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.9/site-packages (4.26.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.venv/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.9/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.9/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from transformers) (3.10.7)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.9/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in ./.venv/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (1.26.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.9/site-packages (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.20.3 in ./.venv/lib/python3.9/site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.9/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: spacy in ./.venv/lib/python3.9/site-packages (3.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./.venv/lib/python3.9/site-packages (from spacy) (1.10.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.9/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in ./.venv/lib/python3.9/site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.9/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in ./.venv/lib/python3.9/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.venv/lib/python3.9/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.9/site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.9/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./.venv/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.9/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./.venv/lib/python3.9/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.9/site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./.venv/lib/python3.9/site-packages (from spacy) (1.24.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.9/site-packages (from spacy) (67.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./.venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./.venv/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./.venv/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->spacy) (2.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: spacy-transformers in ./.venv/lib/python3.9/site-packages (1.2.2)\n",
      "Requirement already satisfied: spacy<4.0.0,>=3.5.0 in ./.venv/lib/python3.9/site-packages (from spacy-transformers) (3.5.1)\n",
      "Requirement already satisfied: transformers<4.27.0,>=3.4.0 in ./.venv/lib/python3.9/site-packages (from spacy-transformers) (4.26.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in ./.venv/lib/python3.9/site-packages (from spacy-transformers) (2.0.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in ./.venv/lib/python3.9/site-packages (from spacy-transformers) (2.4.6)\n",
      "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in ./.venv/lib/python3.9/site-packages (from spacy-transformers) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./.venv/lib/python3.9/site-packages (from spacy-transformers) (1.24.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.9/site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.28.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./.venv/lib/python3.9/site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (6.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (23.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.venv/lib/python3.9/site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (4.65.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.9/site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.9/site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.0.8)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.9/site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (67.6.0)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.1.2)\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./.venv/lib/python3.9/site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (0.10.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.9/site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.9/site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./.venv/lib/python3.9/site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.10.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.9/site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.1.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.9/site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.9/site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.0.4)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in ./.venv/lib/python3.9/site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (8.1.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.9/site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.0.12)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in ./.venv/lib/python3.9/site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.9/site-packages (from torch>=1.8.0->spacy-transformers) (4.5.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from torch>=1.8.0->spacy-transformers) (3.10.7)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.9/site-packages (from torch>=1.8.0->spacy-transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch>=1.8.0->spacy-transformers) (3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.9/site-packages (from transformers<4.27.0,>=3.4.0->spacy-transformers) (2023.3.23)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in ./.venv/lib/python3.9/site-packages (from transformers<4.27.0,>=3.4.0->spacy-transformers) (0.13.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.venv/lib/python3.9/site-packages (from transformers<4.27.0,>=3.4.0->spacy-transformers) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.9/site-packages (from transformers<4.27.0,>=3.4.0->spacy-transformers) (6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (2022.12.7)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./.venv/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./.venv/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->spacy<4.0.0,>=3.5.0->spacy-transformers) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.9/site-packages (from sympy->torch>=1.8.0->spacy-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tabulate in ./.venv/lib/python3.9/site-packages (0.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "Collecting dill>=0.3.6\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Installing collected packages: dill, multiprocess\n",
      "Successfully installed dill-0.3.6 multiprocess-0.70.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install presidio_analyzer\n",
    "%pip install presidio_anonymizer\n",
    "%pip install transformers\n",
    "%pip install pandas\n",
    "%pip install spacy\n",
    "%pip install spacy-transformers\n",
    "%pip install tabulate\n",
    "%pip install multiprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTALL SIMPLE SPACY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTALL COMPLEX SPACY MODEL (ONLY IF YOU USE THIS INSTEAD OF BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine, RecognizerResult, RecognizerRegistry, BatchAnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
    "from presidio_analyzer.nlp_engine import NlpArtifacts\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "import pandas as pd\n",
    "from transformers_rec import (\n",
    "    TransformersRecognizer,\n",
    "    BERT_DEID_CONFIGURATION,\n",
    ")\n",
    "from typing import List, Iterator, Tuple\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import warnings\n",
    "from tabulate import tabulate\n",
    "import os \n",
    "import time\n",
    "from datetime import datetime\n",
    "import psutil\n",
    "import multiprocess  as mp\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING CUDA FOR GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE ANALYZER AND ANONYMIZE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzer_engine(model_path):\n",
    "  \"\"\"Return AnalyzerEngine.\n",
    "    :param model_path: Which model to use for NER:\n",
    "        \"obi/deid_roberta_i2b2\",\n",
    "        \"en_core_web_lg\"\n",
    "    \"\"\"\n",
    "  registry = RecognizerRegistry()\n",
    "  registry.load_predefined_recognizers()\n",
    "  if model_path == \"en_core_web_lg\":\n",
    "    nlp_configuration = {\n",
    "        \"nlp_engine_name\": \"spacy\",\n",
    "        \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"}],\n",
    "    }\n",
    "  else:\n",
    "    # Using a small spaCy model + a HF NER model\n",
    "    transformers_recognizer = TransformersRecognizer(model_path=model_path)\n",
    "    if model_path == \"obi/deid_roberta_i2b2\":\n",
    "      transformers_recognizer.load_transformer(**BERT_DEID_CONFIGURATION)\n",
    "    # Use small spaCy model, no need for both spacy and HF models\n",
    "    # The transformers model is used here as a recognizer, not as an NlpEngine\n",
    "    nlp_configuration = {\n",
    "      \"nlp_engine_name\": \"spacy\",\n",
    "      \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_sm\"}],\n",
    "    }\n",
    "    registry.add_recognizer(transformers_recognizer)\n",
    "\n",
    "  nlp_engine = NlpEngineProvider(nlp_configuration=nlp_configuration).create_engine()\n",
    "\n",
    "  analyzer = AnalyzerEngine(nlp_engine=nlp_engine, registry=registry)\n",
    "  return analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(analyzer, **kwargs):\n",
    "    \"\"\"Analyze input using Analyzer engine and input arguments (kwargs).\"\"\"\n",
    "    if \"entities\" not in kwargs or \"All\" in kwargs[\"entities\"]:\n",
    "        kwargs[\"entities\"] = None\n",
    "    return analyzer.analyze(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymize(text: str, analyze_results: List[RecognizerResult]):\n",
    "    \"\"\"Anonymize identified input using Presidio Anonymizer.\n",
    "    :param text: Full text\n",
    "    :param analyze_results: list of results from presidio analyzer engine\n",
    "    \"\"\"\n",
    "    operator_config = {\"lambda\": lambda x: x}\n",
    "    operator = \"custom\"\n",
    "    res = AnonymizerEngine().anonymize(\n",
    "        text,\n",
    "        analyze_results,\n",
    "        operators={\"DEFAULT\": OperatorConfig(operator, operator_config)},\n",
    "    )\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIAL CONFIG FOR THE ANALYZER AND MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = analyzer_engine(\"obi/deid_roberta_i2b2\") # \"en_core_web_lg\" or \"obi/deid_roberta_i2b2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.40\n",
    "entities = [\"PERSON\", \"LOCATION\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\",\"CREDIT_CARD\", \"US_SSN\"]\n",
    "columns = [\"SITE_URL\",\"PVID\",\"REVIEW\"]\n",
    "number_column_review = 2\n",
    "check_overlaps=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x): \n",
    "  return x*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x*x\n",
    "\n",
    "with mp.Pool(5) as pool:\n",
    "    print(pool.map(f, [1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_obj(an_r, text):\n",
    "    \"\"\"Show results of analyze() in a dataframe.\"\"\"\n",
    "    ents = []\n",
    "    for r in an_r:\n",
    "      info = r.to_dict()\n",
    "      ent ={ \"start\": info[\"start\"], \n",
    "              \"end\": info['end'], \n",
    "              \"confidence\": info['score'], \n",
    "              \"entity\": info['entity_type'], \n",
    "              \"text\": text[info[\"start\"]:info[\"end\"]]} \n",
    "      ents.append(ent)\n",
    "    return ents\n",
    "\n",
    "\n",
    "def model_results(input_path, output_path, entities, threshold, analyzer,columns, number_column_review, check_overlaps=False, ):\n",
    "  final_result = []\n",
    "  df = pd.read_csv(input_path, encoding=\"ISO-8859-1\",header=0, names=columns)\n",
    "  # Extract the review column\n",
    "  # print(df.values.tolist()[0:3])\n",
    "  all_values = df.values.tolist()\n",
    "  data_list = df.iloc[:, number_column_review].tolist()\n",
    "  print(\"getting results for the ner model...\")\n",
    "  start_time = time.time()\n",
    "  results = batch_analyze(analyzer, data_list, entities=entities, language=\"en\", score_threshold=threshold)\n",
    "  print(f'end of run: {time.time()-start_time}')\n",
    "  print(u'Used Memory：%.4f GB' % (psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024 / 1024))\n",
    "  print(\"Anonymizing results (resolve overlaps)...\")\n",
    "  for index, result in tqdm(enumerate(results), total=len(results)):\n",
    "    if check_overlaps:\n",
    "      text_anon = anonymize(data_list[index], result)\n",
    "      text_anon = sorted(text_anon.items, key=lambda x: x.start)\n",
    "      result = []\n",
    "      for i, res in enumerate(text_anon):\n",
    "          result.append({\"start\": res.start, \"end\": res.end, \"entity\": res.entity_type, \"text\": res.text})\n",
    "    else:\n",
    "      result = create_obj(result, data_list[index])\n",
    "    final_result.append({\"SITE_URL\": all_values[index][0],\"PVID\": all_values[index][1], \"TEXT\": data_list[index], \"ENTITIES\": result})\n",
    "  print(\"Saving results to json file...\")\n",
    "  fp=open(output_path,'w', encoding=\"ISO-8859-1\") # output file\n",
    "  json.dump(final_result, fp) \n",
    "  print(\"Done!\") \n",
    "\n",
    "def process_review(all_values):\n",
    "    data_list = [row[number_column_review] for row in all_values]\n",
    "    nlp_artifacts_batch: Iterator[\n",
    "          Tuple[str, NlpArtifacts]\n",
    "      ] = analyzer.nlp_engine.process_batch(\n",
    "          texts=data_list, language=\"en\"\n",
    "      )\n",
    "    final_result = []\n",
    "    for i, (text, nlp_artifacts) in tqdm(enumerate(nlp_artifacts_batch), total=len(data_list)):\n",
    "        results = analyzer.analyze(\n",
    "            text=str(text), nlp_artifacts=nlp_artifacts, language=\"en\", entities=entities, score_threshold=threshold\n",
    "        )\n",
    "        result = []\n",
    "        if check_overlaps:\n",
    "            text_anon = anonymize(data_list[i], result)\n",
    "            text_anon = sorted(text_anon.items, key=lambda x: x.start)\n",
    "            for i, res in enumerate(text_anon):\n",
    "                result.append({\"start\": res.start, \"end\": res.end, \"entity\": res.entity_type, \"text\": res.text})\n",
    "        else:\n",
    "            result = create_obj(result, data_list[i])\n",
    "        final_result.append({\"SITE_URL\": all_values[i][0],\"PVID\": all_values[i][1], \"TEXT\": data_list[i], \"ENTITIES\": result})\n",
    "    return final_result\n",
    "\n",
    "\n",
    "def process_review2(value):\n",
    "    \n",
    "    review = value[number_column_review]\n",
    "    final_result = []\n",
    "    results = analyzer.analyze(\n",
    "        text=str(review), language=\"en\", entities=entities, score_threshold=threshold\n",
    "    )\n",
    "    result = []\n",
    "    if check_overlaps:\n",
    "        text_anon = anonymize(review, result)\n",
    "        text_anon = sorted(text_anon.items, key=lambda x: x.start)\n",
    "        for i, res in enumerate(text_anon):\n",
    "            result.append({\"start\": res.start, \"end\": res.end, \"entity\": res.entity_type, \"text\": res.text})\n",
    "    else:\n",
    "        result = create_obj(result, review)\n",
    "    final_result.append({\"SITE_URL\": value[0],\"PVID\": value[1], \"TEXT\": review, \"ENTITIES\": result})\n",
    "    return final_result\n",
    "\n",
    "def model_results_parallel(input_path, output_path):\n",
    "    # Read input data\n",
    "    df = pd.read_csv(input_path, encoding=\"ISO-8859-1\", header=0, names=columns)\n",
    "    # Extract the review column\n",
    "    all_values = df.values.tolist()\n",
    "\n",
    "    # Create pool of worker processes\n",
    "    num_workers = mp.cpu_count() - 1\n",
    "    pool = mp.Pool(num_workers)\n",
    "\n",
    "\n",
    "    print(f\"Getting results for the NER model and Presidio Analyzer using {num_workers} workers...\")\n",
    "    # Create a tqdm instance to track progress\n",
    "    pbar = tqdm(total=len(all_values))\n",
    "    # results = Parallel(n_jobs=num_workers)(delayed(process_review2)(x) for x in all_values)\n",
    "    with pool as p:\n",
    "        results = p.map(process_review2, all_values)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    print(\"Saving results to json file...\")\n",
    "    with open(output_path, 'w', encoding=\"ISO-8859-1\") as fp:\n",
    "        json.dump(results, fp)\n",
    "    print(\"Done!\")\n",
    "\n",
    "\n",
    "    # # Use the joblib.Parallel context manager with tqdm\n",
    "    # with Parallel(n_jobs=num_workers) as parallel:\n",
    "    #     # Call delayed with the function and the iterable\n",
    "    #     results = parallel(delayed(process_review2)(np.copy(x)) for x in all_values)\n",
    "    #     # Update the progress bar\n",
    "    #     pbar.update()\n",
    "\n",
    "    # Close the progress bar\n",
    "    # pbar.close()\n",
    "    \n",
    "    # print(f\"Getting results for the NER model and Presidio Analyzer...\")\n",
    "    # results = process_review(analyzer, all_values, entities, threshold, check_overlaps, number_column_review)\n",
    "    # print(\"Saving results to json file...\")\n",
    "    # with open(output_path, 'w', encoding=\"ISO-8859-1\") as fp:\n",
    "    #     json.dump(results, fp)\n",
    "    # print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST SIMPLE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(text: str, analyze_results: List[RecognizerResult]):\n",
    "    \"\"\"\n",
    "    Highlights every identified entity on top of the text.\n",
    "    :param text: full text\n",
    "    :param analyze_results: list of analyzer results.\n",
    "    \"\"\"\n",
    "    ents = []\n",
    "\n",
    "    # Use the anonymizer to resolve overlaps\n",
    "    results = anonymize(text, analyze_results)\n",
    "    # sort by start index\n",
    "    results = sorted(results.items, key=lambda x: x.start)\n",
    "    for i, res in enumerate(results):\n",
    "        ents.append({\"start\": res.start, \"end\": res.end, \"label\": res.entity_type, \"text\": res.text})\n",
    "    return [{\"text\": text, \"ents\": ents}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(an_r, text, return_analyzer_results=False):\n",
    "    \"\"\"Show results of analyze() in a dataframe.\"\"\"\n",
    "    df = pd.DataFrame.from_records([r.to_dict() for r in an_r])\n",
    "    df[\"text\"] = [text[res.start: res.end] for res in an_r]\n",
    "    df_subset = df[[\"entity_type\", \"text\", \"start\", \"end\", \"score\"]].rename(\n",
    "        {\n",
    "            \"entity_type\": \"Entity type\",\n",
    "            \"text\": \"Text\",\n",
    "            \"start\": \"Start\",\n",
    "            \"end\": \"End\",\n",
    "            \"score\": \"Confidence\",\n",
    "        },\n",
    "        axis=1,\n",
    "    )\n",
    "    df_subset[\"Text\"] = [text[res.start: res.end] for res in an_r]\n",
    "    #  In analysis_explanation_df there are more columns than in df_subset with more information. \n",
    "    if return_analyzer_results:\n",
    "      analysis_explanation_df = pd.DataFrame.from_records(\n",
    "          [r.analysis_explanation.to_dict() for r in an_r]\n",
    "      )\n",
    "    # df_subset = pd.concat([df_subset, analysis_explanation_df], axis=1)\n",
    "    result = annotate(text, an_r)\n",
    "    return df_subset.reset_index(drop=True), result\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"My name is Gonzalo Zelinka, I'm from Argentina and I live in Barcelona. My phone number is +34 666 666 666 and my email is gonzalozelinka@gmail.com. I work at Microsoft and my credit card number is 1234 5678 9012 3456. My SSN is 123-45-6789.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_results = analyze(\n",
    "    analyzer_engine=analyzer,\n",
    "    texts=text,\n",
    "    entities= entities,\n",
    "    language=\"en\",\n",
    "    score_threshold=threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame, sentence = show_results(analyze_results, text)\n",
    "# print(sentence)\n",
    "displacy.render(sentence, style=\"ent\", manual=True)\n",
    "display(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERATE RESULTS FROM COMPLEX DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for the NER model and Presidio Analyzer using 9 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[AProcess ForkPoolWorker-129:\n",
      "Process ForkPoolWorker-128:\n",
      "Process ForkPoolWorker-126:\n",
      "Process ForkPoolWorker-127:\n",
      "Process ForkPoolWorker-124:\n",
      "Process ForkPoolWorker-130:\n",
      "Process ForkPoolWorker-125:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# input_path, output_path, entities, threshold, analyzer, columns, number_column_review, check_overlaps=False\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel_results_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtesting-data/sentences_from_db copy.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtesting-data/testing.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 101\u001b[0m, in \u001b[0;36mmodel_results_parallel\u001b[0;34m(input_path, output_path)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# results = Parallel(n_jobs=num_workers)(delayed(process_review2)(x) for x in all_values)\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pool \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m--> 101\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_review2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    103\u001b[0m pbar\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Desktop/PII_POC/.venv/lib/python3.9/site-packages/multiprocess/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/Desktop/PII_POC/.venv/lib/python3.9/site-packages/multiprocess/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    766\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/PII_POC/.venv/lib/python3.9/site-packages/multiprocess/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py:574\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    572\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    573\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 574\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    575\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# input_path, output_path, entities, threshold, analyzer, columns, number_column_review, check_overlaps=False\n",
    "model_results_parallel(\"testing-data/sentences_from_db copy.csv\", \"testing-data/testing.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATE MODEL\n",
    "The format to the dataset if like this: \n",
    "- [Example Dataset](https://www.kaggle.com/datasets/namanj27/ner-dataset)\n",
    "\n",
    "Column necessaries and their names:\n",
    "- Sentence # --> Review #\n",
    "- Word --> Word\n",
    "- POS --> Delete this.\n",
    "- Tag --> Tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT THE GROUND TRUTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span_indx(\n",
    "    labels: List[str],\n",
    "    words: List[str],\n",
    "    sentence: str\n",
    ") -> List[tuple]:\n",
    "    \"\"\"Gets span starts and ends for Spacy spancat component.\n",
    "        \n",
    "        Returns list of tuples where the first element of the \n",
    "        tuple is the span start, the second element of the tuple\n",
    "        is the span end and the third element of the tuple is\n",
    "        the span category. \n",
    "    \"\"\"\n",
    "    #gets list of indices corresponding to labelled words \n",
    "    label_indx = []\n",
    "    temp_list = []\n",
    "\n",
    "    for i, l in enumerate(labels):\n",
    "        if l != 'O':\n",
    "            temp_list.append(i)\n",
    "        else:\n",
    "            label_indx.append(temp_list)\n",
    "            temp_list = []    \n",
    "        if i == len(labels) - 1:\n",
    "            label_indx.append(temp_list)\n",
    "\n",
    "    clean_label_indx = [x for x in label_indx if len(x) > 0]\n",
    "\n",
    "    spans = []\n",
    "    for indx in clean_label_indx:\n",
    "        if len(indx) == 1:\n",
    "            span = words[indx[0]]\n",
    "            label = labels[indx[0]].upper()\n",
    "        else:\n",
    "            span = ' '.join([words[i] for i in indx])  \n",
    "            label = [labels[i].upper() for i in indx][0]\n",
    "        #remove punctuation and strip whitespace for spans\n",
    "        span_clean = span.strip()\n",
    "        for m in re.finditer(re.escape(span_clean), sentence):\n",
    "            spans.append({\"start\":m.start(), \"end\":m.end(), \"entity\": label, \"text\": m.group()})\n",
    "    \n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_csv_annotated_to_json(input_path):\n",
    "    DATA = []\n",
    "    data = (pd.read_csv(input_path, encoding='ISO-8859-1')\n",
    "          .fillna(method='ffill'))\n",
    "    for sent, sent_info in data.groupby('Review #'):\n",
    "      words = list(sent_info[\"Word\"])\n",
    "      #convert words to sentence and get rid of spaces between punctuation characters\n",
    "      sentence = re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', \" \".join(words))\n",
    "      #get labels\n",
    "      labels = list(sent_info['Tag'])\n",
    "      #identify token span start, span ends and span category\n",
    "      span_ents = get_span_indx(labels, words, sentence)\n",
    "      DATA.append({\"TEXT\": sentence, \"ENTITIES\": span_ents})\n",
    "    return DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = transform_csv_annotated_to_json(\"testing-data/product_reviews9.csv\")\n",
    "with open('testing-data/true_data.json', 'w') as fp:\n",
    "    json.dump(ground_truth, fp)\n",
    "print(ground_truth[1999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT ONLY SENTENCES TO SEND TO THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_to_eval(input_file, output_file):\n",
    "  data = (pd.read_csv(input_file, encoding='ISO-8859-1')\n",
    "    .fillna(method='ffill'))\n",
    "  with open(output_file, 'w', encoding='ISO-8859-1') as fo:\n",
    "    writer = csv.writer(fo)\n",
    "    writer.writerow(['SENTENCES']) \n",
    "    for sent, sent_info in data.groupby('Review #'):\n",
    "      words = list(sent_info[\"Word\"])\n",
    "      sentence = re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', \" \".join(words))\n",
    "      writer.writerow([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_sentences_to_eval(\"testing-data/product_reviews9.csv\", \"testing-data/sentences_evaluate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_from_db(input_file, output_file):\n",
    "  data = (pd.read_csv(input_file, encoding='ISO-8859-1')\n",
    "    .fillna(method='ffill'))\n",
    "  reviews = []\n",
    "  for idx, row in data.iterrows():\n",
    "    for review in json.loads(row[\"REVIEWS\"]):\n",
    "      reviews.append([row[\"SITE_URL\"], row[\"PVID\"], review])\n",
    "  new_data = pd.DataFrame(reviews, columns=[\"SITE_URL\", \"PVID\", \"REVIEW\"])\n",
    "  new_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_sentences_from_db(\"testing-data/Flamingo_Reviews_Extracted_Data_2023_04_12 (1).csv\", \"testing-data/sentences_from_db.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN THE MODEL WITH THE EXTRACTED SENTENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results(input_path=\"testing-data/input-path.csv\", output_path=\"testing-data/output-path.json\", \n",
    "entities=entities, threshold=threshold, analyzer=analyzer, columns=[\"SITE_URL\", \"PVID\", \"REVIEW\"], check_overlaps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REMOVE UNWANTED ENTITIES FROM THE MODEL RESULTS AND JOIN ENTITIES SUBDIVIDED INTO ONE\n",
    "- Sometimes the model divide entities into several entities, so we need to join them to evaluate the results. For example, if the model found the entity \"686 E Broadway\" as \"6\", \"86 E\" and \"Broadway\", we need to join them to evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_entities_to_eval(input_path, output_path):\n",
    "    # Read the input JSON file\n",
    "    with open(input_path, \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "        json_data = f.read()\n",
    "    # Load the JSON data into a Python list\n",
    "    data_to_check = json.loads(json_data)\n",
    "    # Loop over each item in the list\n",
    "    for js in data_to_check:\n",
    "        # Remove entities with \"O\" entity value\n",
    "        js[\"ENTITIES\"] = [ent for ent in js[\"ENTITIES\"] if ent[\"entity\"] != \"O\"]\n",
    "\n",
    "        # Fix overlapping entities and combine entities with a single character distance\n",
    "        entities_clean = []\n",
    "        i = 0 \n",
    "        # Loop over each entity in the current item\n",
    "        # print(\"js[ENTITIES]: \", js[\"ENTITIES\"])\n",
    "        while i < len(js[\"ENTITIES\"]):\n",
    "            j = i + 1\n",
    "            new_entity = js[\"ENTITIES\"][i]\n",
    "            # Combine adjacent entities that have the same type and are next to each other\n",
    "            while j < len(js[\"ENTITIES\"]):\n",
    "                if (js[\"ENTITIES\"][j]['entity'] != js[\"ENTITIES\"][i]['entity'] \n",
    "                    or int(js[\"ENTITIES\"][j][\"start\"]) - int(js[\"ENTITIES\"][i][\"end\"]) > 1):\n",
    "                    # If the next entity is not the same type or is not adjacent, stop combining entities\n",
    "                    break\n",
    "                new_entity[\"end\"] = int(js[\"ENTITIES\"][j][\"end\"])\n",
    "                # print(\"new_entity end: \", new_entity[\"end\"])\n",
    "                new_entity[\"text\"] = new_entity[\"text\"] + \" \" + js[\"ENTITIES\"][j][\"text\"]\n",
    "                # print(\"new_entity text: \", new_entity[\"text\"])\n",
    "                j += 1\n",
    "\n",
    "            entities_clean.append(new_entity)\n",
    "            i = j\n",
    "        # Update the current item with the cleaned entities\n",
    "        # print(\"entities_clean final: \", entities_clean)\n",
    "        js[\"ENTITIES\"] = entities_clean\n",
    "    # Write the updated JSON data to the output file\n",
    "    with open(output_path, 'w', encoding=\"ISO-8859-1\") as fp:\n",
    "        json.dump(data_to_check, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_entities_to_eval(\"testing-data/output_lg.json\", \"testing-data/output_lg_fixed.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testing-data/output_lg_fixed.json\", \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "  json_data = f.read()\n",
    "prediction_data = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_general_scores(entity_scores):\n",
    "    precision_sum = 0\n",
    "    recall_sum = 0\n",
    "    f1_sum = 0\n",
    "    for entity, scores in entity_scores.items():\n",
    "        precision_sum += scores['precision']\n",
    "        recall_sum += scores['recall']\n",
    "        f1_sum += scores['f1']\n",
    "\n",
    "    num_entities = len(entity_scores)\n",
    "    general_precision = precision_sum / num_entities\n",
    "    general_recall = recall_sum / num_entities\n",
    "    general_f1 = f1_sum / num_entities\n",
    "\n",
    "    return general_precision, general_recall, general_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE CASE: EXACT\n",
    "- Identifies the exact words associated with all PII entities in the input text.\n",
    "- This use case is applicable if the client wants to know which exact words correspond to the PII information. For example to apply masks over the PII entities detected in the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall_for_entities(ground_truth, output_model, unique_entities, output_file):\n",
    "    # Initialize counters for true positives, false positives, and false negatives\n",
    "    tp = {entity: 0 for entity in unique_entities}\n",
    "    fp = {entity: 0 for entity in unique_entities}\n",
    "    fn = {entity: 0 for entity in unique_entities}\n",
    "\n",
    "    for example_idx in range(len(ground_truth)):\n",
    "        ground_truth_entities = ground_truth[example_idx]['ENTITIES']\n",
    "        output_entities = output_model[example_idx]['ENTITIES']\n",
    "        # create a set of output entity texts for quick lookup\n",
    "        ground_truth_texts = set([ent['text'].replace(\" \", \"\") for ent in ground_truth_entities])\n",
    "\n",
    "        for ground_truth_ent in ground_truth_entities:\n",
    "            ent_type = ground_truth_ent['entity']\n",
    "            if ent_type not in unique_entities:\n",
    "                continue\n",
    "            # print(output_entities)\n",
    "            if any([ent['text'].replace(\" \", \"\") == ground_truth_ent['text'].replace(\" \",\"\") and ent[\"start\"] == ground_truth_ent[\"start\"] for ent in output_entities]):\n",
    "                tp[ent_type] += 1\n",
    "            else:\n",
    "                fn[ent_type] += 1\n",
    "        for output_ent in output_entities:\n",
    "            ent_type = output_ent['entity']\n",
    "            if ent_type not in unique_entities:\n",
    "                continue\n",
    "            if output_ent['text'].replace(\" \",\"\") not in ground_truth_texts:\n",
    "                fp[ent_type] += 1\n",
    "    # Calculate precision, recall, and F1 score for each entity\n",
    "    scores = {}\n",
    "    table = []\n",
    "    headers = ['Entity', 'Precision', 'Recall', 'F1']\n",
    "    for entity in unique_entities:\n",
    "        p = round(tp[entity] / (tp[entity] + fp[entity]), 2) if tp[entity] + fp[entity] > 0 else 0\n",
    "        r = round(tp[entity] / (tp[entity] + fn[entity]), 2) if tp[entity] + fn[entity] > 0 else 0\n",
    "        f1 = round(2 * p * r / (p + r), 2) if p + r > 0 else 0\n",
    "        scores[entity] = {'precision': p, 'recall': r, 'f1': f1}\n",
    "        table.append([entity, p, r, f1])\n",
    "    \n",
    "    general_precision, general_recall, general_f1 = calculate_general_scores(scores)\n",
    "    report = f\"\\nGeneral Precision: {general_precision}\\nGeneral Recall: {general_recall}\\nGeneral F1: {general_f1}\\n{tabulate(table, headers)}\"\n",
    "    with open(output_file, 'w', encoding='ISO-8859-1') as f:\n",
    "        f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_precision_recall_for_entities(ground_truth, prediction_data,set([\"PERSON\", \"EMAIL_ADDRESS\", \"LOCATION\", \"US_SSN\", \"CREDIT_CARD\", \"PHONE_NUMBER\"]) , \"eval/REPORT_LG2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE CASE: BINARY\n",
    "- Given an input text, it indicates whether each entity (person, location, credit_card, phone_number, us_ssn) is present at least once or not.\n",
    "- The use case is applicable for filtering out reviews with sensitive information without needing to know which part of the text has the sensitive information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_entities(ground_truth, output_model, unique_entities, output_file):\n",
    "    # Initialize counters for true positives, false positives, and false negatives\n",
    "    tp = {entity: 0 for entity in unique_entities}\n",
    "    fp = {entity: 0 for entity in unique_entities}\n",
    "    fn = {entity: 0 for entity in unique_entities}\n",
    "    \n",
    "    # Loop over the documents in the output model\n",
    "    for i, doc in enumerate(output_model):\n",
    "        # Get the set of entities present in the document\n",
    "        model_entity_set = set()\n",
    "        for entity in doc['ENTITIES']:\n",
    "            model_entity_set.add(entity['entity'])\n",
    "        # Get the set of entities present in the ground truth in the same index of output model\n",
    "        ground_truth_set = set()\n",
    "        for entity in ground_truth[i]['ENTITIES']:\n",
    "            ground_truth_set.add(entity['entity'])\n",
    "            \n",
    "        # Check if each entity in the ground truth is present in the output model\n",
    "        for entity in ground_truth_set:\n",
    "            if entity in model_entity_set:\n",
    "                # Entity is present in both ground truth and output model\n",
    "                tp[entity] += 1\n",
    "            else:\n",
    "                # Entity is present in ground truth but not in output model\n",
    "                fn[entity] += 1\n",
    "        \n",
    "        # Check if each entity in the output model is a false positive\n",
    "        for entity in model_entity_set:\n",
    "            if entity not in ground_truth_set:\n",
    "                # Entity is not in the ground truth\n",
    "                fp[entity] += 1\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score for each entity\n",
    "    scores = {}\n",
    "    table = []\n",
    "    headers = ['Entity', 'Precision', 'Recall', 'F1']\n",
    "    for entity in unique_entities:\n",
    "        p = round(tp[entity] / (tp[entity] + fp[entity]), 2) if tp[entity] + fp[entity] > 0 else 0\n",
    "        r = round(tp[entity] / (tp[entity] + fn[entity]), 2) if tp[entity] + fn[entity] > 0 else 0\n",
    "        f1 = round(2 * p * r / (p + r), 2) if p + r > 0 else 0\n",
    "        scores[entity] = {'precision': p, 'recall': r, 'f1': f1}\n",
    "        table.append([entity, p, r, f1])\n",
    "    \n",
    "    general_precision, general_recall, general_f1 = calculate_general_scores(scores)\n",
    "    report = f\"\\nGeneral Precision: {general_precision}\\nGeneral Recall: {general_recall}\\nGeneral F1: {general_f1}\\n{tabulate(table, headers)}\"\n",
    "    with open(output_file, 'w', encoding='ISO-8859-1') as f:\n",
    "        f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1 = [{'TEXT': 'a1', \"ENTITIES\": [{'start': 21, 'end': 33, 'entity': 'PERSON', 'text': 'Zoey Edwards'}, {'start': 129, 'end': 151, 'entity': 'EMAIL_ADDRESS', 'text': 'edwards-zoey@gmail.com'}, {'start': 190, 'end': 201, 'entity': 'LOCATION', 'text': '900 F St NW'}, {'start': 213, 'end': 222, 'entity': 'US_SSN', 'text': '367245504'}, {'start': 252, 'end': 271, 'entity': 'CREDIT_CARD', 'text': '2259-8740-7030-1462'}]}]\n",
    "# a2 = [{\"TEXT\": \"a2\", \"ENTITIES\": [{'start': 21, 'end': 33, 'entity': 'PERSON', 'text': 'Zoey Edwards'}, {'start': 129, 'end': 151, 'entity': 'EMAIL_ADDRESS', 'text': 'edwards-zoey@gmail.com'}, {'start': 190, 'end': 195, 'entity': 'LOCATION', 'text': '900 F'}, {'start': 196, 'end': 201, 'entity': 'LOCATION', 'text': 'St NW'}, {'start': 213, 'end': 222, 'entity': 'US_SSN', 'text': '367245504'}, {'start': 252, 'end': 255, 'entity': 'PHONE_NUMBER', 'text': '225'}, {'start': 255, 'end': 271, 'entity': 'PHONE_NUMBER', 'text': '9-8740-7030-1462'}]}]\n",
    "evaluate_entities(ground_truth, prediction_data, set([\"PERSON\", \"EMAIL_ADDRESS\", \"LOCATION\", \"US_SSN\", \"CREDIT_CARD\", \"PHONE_NUMBER\"]), \"eval/REPORT_SENTENCE_LG.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a9f895cc7e3ca0d623982e8f7234235464dc9c0b943e7d04631c8d05e4aca2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
