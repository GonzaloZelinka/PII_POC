{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTALL DEPENDENCIES\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR MAC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR CUDA (GPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install presidio_analyzer\n",
    "%pip install presidio_anonymizer\n",
    "%pip install transformers\n",
    "%pip install pandas\n",
    "%pip install spacy\n",
    "%pip install spacy-transformers\n",
    "%pip install tabulate\n",
    "%pip install multiprocess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTALL SIMPLE SPACY MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTALL COMPLEX SPACY MODEL (ONLY IF YOU USE THIS INSTEAD OF BERT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "from presidio_analyzer import RecognizerResult\n",
    "from spacy import displacy\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "from transformers_rec import (\n",
    "    Analyzer,\n",
    "    Anonymizer,\n",
    "    CustomAnalyzerEngine,\n",
    ")\n",
    "from utils import fix_entities, model_results, transform_csv_annotated_to_json\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING CUDA FOR GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST SIMPLE DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(text: str, analyze_results: List[RecognizerResult]):\n",
    "    \"\"\"\n",
    "    Highlights every identified entity on top of the text.\n",
    "    :param text: full text\n",
    "    :param analyze_results: list of analyzer results.\n",
    "    \"\"\"\n",
    "    ents = []\n",
    "\n",
    "    # Use the anonymizer to resolve overlaps\n",
    "    anonymizer = Anonymizer(type=\"simple\")\n",
    "    results = anonymizer.anonymize(text=text, analyze_results=analyze_results)\n",
    "    # sort by start index\n",
    "    results = sorted(results.items, key=lambda x: x.start)\n",
    "    for i, res in enumerate(results):\n",
    "        ents.append(\n",
    "            {\n",
    "                \"start\": res.start,\n",
    "                \"end\": res.end,\n",
    "                \"label\": res.entity_type,\n",
    "                \"text\": res.text,\n",
    "            }\n",
    "        )\n",
    "    return [{\"text\": text, \"ents\": ents}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(an_r, text, analyzer, return_analyzer_results=False ):\n",
    "    \"\"\"Show results of analyze() in a dataframe.\"\"\"\n",
    "    if (isinstance(analyzer.get_engine(), CustomAnalyzerEngine)):\n",
    "        df = pd.DataFrame.from_records([r.to_dict() for r in an_r])\n",
    "        df[\"text\"] = [text[res.start : res.end] for res in an_r]\n",
    "        df_subset = df[[\"entity_type\", \"text\", \"start\", \"end\", \"score\"]].rename(\n",
    "            {\n",
    "                \"entity_type\": \"Entity type\",\n",
    "                \"text\": \"Text\",\n",
    "                \"start\": \"Start\",\n",
    "                \"end\": \"End\",\n",
    "                \"score\": \"Confidence\",\n",
    "            },\n",
    "            axis=1,\n",
    "        )\n",
    "        df_subset[\"Text\"] = [text[res.start : res.end] for res in an_r]\n",
    "        #  In analysis_explanation_df there are more columns than in df_subset with more information.\n",
    "        if return_analyzer_results:\n",
    "            analysis_explanation_df = pd.DataFrame.from_records(\n",
    "                [r.analysis_explanation.to_dict() for r in an_r]\n",
    "            )\n",
    "        # df_subset = pd.concat([df_subset, analysis_explanation_df], axis=1)\n",
    "        result = annotate(text, an_r)\n",
    "        return [(df_subset.reset_index(drop=True), result)]\n",
    "    else:\n",
    "        results = []\n",
    "        for result, text in zip(an_r, text):\n",
    "            df = pd.DataFrame.from_records([r.to_dict() for r in result])\n",
    "            df[\"text\"] = [text[res.start : res.end] for res in result]\n",
    "            df_subset = df[[\"entity_type\", \"text\", \"start\", \"end\", \"score\"]].rename(\n",
    "                {\n",
    "                    \"entity_type\": \"Entity type\",\n",
    "                    \"text\": \"Text\",\n",
    "                    \"start\": \"Start\",\n",
    "                    \"end\": \"End\",\n",
    "                    \"score\": \"Confidence\",\n",
    "                },\n",
    "                axis=1,\n",
    "            )\n",
    "            df_subset[\"Text\"] = [text[res.start : res.end] for res in result]\n",
    "            #  In analysis_explanation_df there are more columns than in df_subset with more information.\n",
    "            if return_analyzer_results:\n",
    "                analysis_explanation_df = pd.DataFrame.from_records(\n",
    "                    [r.analysis_explanation.to_dict() for r in result]\n",
    "                )\n",
    "            # df_subset = pd.concat([df_subset, analysis_explanation_df], axis=1)\n",
    "            result = annotate(text, result)\n",
    "            results.append((df_subset.reset_index(drop=True), result))\n",
    "        return results\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMPLE TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = Analyzer(\n",
    "    \"obi/deid_roberta_i2b2\", type=\"simple\"\n",
    ")  # \"en_core_web_lg\" or \"obi/deid_roberta_i2b2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My name is John Smith and I live in New York. My phone number is 212-555-5555 and my email is jhonsmith@gmail.com and my credit card is 4130-2046-7153-3043 and my SSN is 136-84-8867. I am 25 years old and I work at Google.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_results = analyzer.analyze(\n",
    "    text=text,\n",
    "    entities=[\n",
    "        \"PERSON\",\n",
    "        \"LOCATION\",\n",
    "        \"PHONE_NUMBER\",\n",
    "        \"EMAIL_ADDRESS\",\n",
    "        \"CREDIT_CARD\",\n",
    "        \"US_SSN\",\n",
    "    ],\n",
    "    language=\"en\",\n",
    "    score_threshold={\n",
    "        \"PERSON\": 0.9,\n",
    "        \"CREDIT_CARD\": 0.5,\n",
    "        \"US_SSN\": 0.3,\n",
    "        \"EMAIL_ADDRESS\": 0.5,\n",
    "        \"PHONE_NUMBER\": 0.4,\n",
    "        \"LOCATION\": 0.7,\n",
    "    },\n",
    ")\n",
    "print(analyze_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BATCH TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = Analyzer(\"obi/deid_roberta_i2b2\", type=\"batch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"My name is John Smith and I live in New York. My phone number is 212-555-5555 and my email is jhonsmith@gmail.com and my credit card is 4130-2046-7153-3043 and my SSN is 136-84-8867. I am 25 years old and I work at Google.\",\n",
    "    \"As usual, my experience with Mission Belt was just fine. If you need more help, text me at 555-3424-5324\",\n",
    "    \"As a customer for 5+ years, we know that a Mission Belt is a great gift, too. Our friend had never seen anything like it. My name is Gonzalo Zelinka\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_results = analyzer.analyze(\n",
    "    texts=text,\n",
    "    language=\"en\",\n",
    "    entities=[\n",
    "        \"PERSON\",\n",
    "        \"LOCATION\",\n",
    "        \"PHONE_NUMBER\",\n",
    "        \"EMAIL_ADDRESS\",\n",
    "        \"CREDIT_CARD\",\n",
    "        \"US_SSN\",\n",
    "    ],\n",
    "    score_threshold={\n",
    "        \"PERSON\": 0.9,\n",
    "        \"CREDIT_CARD\": 0.5,\n",
    "        \"US_SSN\": 0.4,\n",
    "        \"EMAIL_ADDRESS\": 0.5,\n",
    "        \"PHONE_NUMBER\": 0.4,\n",
    "        \"LOCATION\": 0.7,\n",
    "    },\n",
    ")\n",
    "print(analyze_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHOW RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(analyze_results) > 0:\n",
    "    results = show_results(analyze_results, text, analyzer=analyzer)\n",
    "    # print(sentence)\n",
    "    for result in results: \n",
    "        frame, sentence = result\n",
    "        displacy.render(sentence, style=\"ent\", manual=True)\n",
    "        display(frame)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACT SENTENCES TO SEND THE MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_to_eval(input_file, output_file, is_list_reviews=True):\n",
    "    data = pd.read_csv(input_file, encoding=\"ISO-8859-1\").fillna(method=\"ffill\")\n",
    "    with open(output_file, \"w\", encoding=\"ISO-8859-1\") as fo:\n",
    "        writer = csv.writer(fo)\n",
    "        num_reviews = 0\n",
    "        for index, row in data.iterrows():\n",
    "            if is_list_reviews:\n",
    "                reviews_list = eval(row[\"REVIEWS\"])\n",
    "                for review in reviews_list:\n",
    "                    num_reviews = num_reviews + 1\n",
    "                    new_row = [\n",
    "                        str(row[\"SITE_URL\"]),\n",
    "                        row[\"ITEM_GROUP_ID\"],\n",
    "                        row[\"PVIDS\"],\n",
    "                        re.sub(r\"[\\n\\r]{1,}\", \" \", str(review)),\n",
    "                    ]\n",
    "                    writer.writerow(new_row)\n",
    "            else:\n",
    "                num_reviews = num_reviews + 1\n",
    "                new_row = [\n",
    "                    str(row[\"SITE_URL\"]),\n",
    "                    row[\"ITEM_GROUP_ID\"],\n",
    "                    re.sub(r\"[\\n\\r]{1,}\", \" \", row[\"PVIDS\"]),\n",
    "                    re.sub(r\"[\\n\\r]{1,}\", \" \", str(row[\"REVIEWS\"])),\n",
    "                ]\n",
    "                writer.writerow(new_row)\n",
    "        return num_reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reviews = extract_sentences_to_eval(\n",
    "    \"testing-data/reviews_from_10_sites.csv\",\n",
    "    \"testing-data/cleaned_sentences_from_db3.csv\",\n",
    "    is_list_reviews=False,\n",
    ")\n",
    "print(f\"Number of reviews: {num_reviews}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because we use n-calls, the warning is annoying as it repeats itself all the time\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\"You seem to be using the pipelines sequentially on GPU.\"\n",
    ")\n",
    "model_results(\n",
    "    input_path=\"testing-data/cleaned_sentences_from_db2.csv\",\n",
    "    output_path=\"testing-data/output_from_db3.csv\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATE MODEL\n",
    "\n",
    "The format to the dataset if like this:\n",
    "\n",
    "- [Example Dataset](https://www.kaggle.com/datasets/namanj27/ner-dataset)\n",
    "\n",
    "Column necessaries and their names:\n",
    "\n",
    "- Sentence # --> Review #\n",
    "- Word --> Word\n",
    "- POS --> Delete this.\n",
    "- Tag --> Tag.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT THE GROUND TRUTH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = transform_csv_annotated_to_json(\"testing-data/product_reviews9.csv\")\n",
    "print(ground_truth[1999])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT ONLY SENTENCES TO SEND TO THE MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_to_eval(input_file, output_file):\n",
    "    data = pd.read_csv(input_file, encoding=\"ISO-8859-1\").fillna(method=\"ffill\")\n",
    "    with open(output_file, \"w\", encoding=\"ISO-8859-1\") as fo:\n",
    "        writer = csv.writer(fo)\n",
    "        writer.writerow([\"SENTENCES\"])\n",
    "        for sent, sent_info in data.groupby(\"Review #\"):\n",
    "            words = list(sent_info[\"Word\"])\n",
    "            sentence = re.sub(r'\\s([?.!\"](?:\\s|$))', r\"\\1\", \" \".join(words))\n",
    "            writer.writerow([sentence])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_sentences_to_eval(\n",
    "    \"testing-data/product_reviews9.csv\", \"testing-data/sentences_evaluate.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_from_db(input_file, output_file):\n",
    "    data = pd.read_csv(input_file, encoding=\"ISO-8859-1\").fillna(method=\"ffill\")\n",
    "    reviews = []\n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        for review in json.loads(row[\"REVIEWS\"]):\n",
    "            reviews.append([row[\"SITE_URL\"], row[\"PVID\"], review])\n",
    "    new_data = pd.DataFrame(reviews, columns=[\"SITE_URL\", \"PVID\", \"REVIEW\"])\n",
    "    new_data.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_sentences_from_db(\n",
    "    \"testing-data/reviews.csv\", \"testing-data/sentences_from_db2.csv\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN THE MODEL WITH THE EXTRACTED SENTENCES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results(\n",
    "    analyzer=analyzer, # Always of type batch\n",
    "    input_path=\"testing-data/cleaned_sentences_from_db3.csv\",\n",
    "    output_path=\"testing-data/output_from_db3.csv\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testing-data/output_path.json\", \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "    json_data = f.read()\n",
    "prediction_data = json.loads(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_general_scores(entity_scores):\n",
    "    precision_sum = 0\n",
    "    recall_sum = 0\n",
    "    f1_sum = 0\n",
    "    for entity, scores in entity_scores.items():\n",
    "        precision_sum += scores[\"precision\"]\n",
    "        recall_sum += scores[\"recall\"]\n",
    "        f1_sum += scores[\"f1\"]\n",
    "\n",
    "    num_entities = len(entity_scores)\n",
    "    general_precision = precision_sum / num_entities\n",
    "    general_recall = recall_sum / num_entities\n",
    "    general_f1 = f1_sum / num_entities\n",
    "\n",
    "    return general_precision, general_recall, general_f1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE CASE: EXACT\n",
    "\n",
    "- Identifies the exact words associated with all PII entities in the input text.\n",
    "- This use case is applicable if the client wants to know which exact words correspond to the PII information. For example to apply masks over the PII entities detected in the input text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall_for_entities(\n",
    "    ground_truth, output_model, unique_entities, output_file\n",
    "):\n",
    "    # Initialize counters for true positives, false positives, and false negatives\n",
    "    tp = {entity: 0 for entity in unique_entities}\n",
    "    fp = {entity: 0 for entity in unique_entities}\n",
    "    fn = {entity: 0 for entity in unique_entities}\n",
    "\n",
    "    for example_idx in range(len(ground_truth)):\n",
    "        ground_truth_entities = ground_truth[example_idx][\"ENTITIES\"]\n",
    "        output_entities = output_model[example_idx][\"ENTITIES\"]\n",
    "        # create a set of output entity texts for quick lookup\n",
    "        ground_truth_texts = set(\n",
    "            [ent[\"text\"].replace(\" \", \"\") for ent in ground_truth_entities]\n",
    "        )\n",
    "\n",
    "        for ground_truth_ent in ground_truth_entities:\n",
    "            ent_type = ground_truth_ent[\"entity\"]\n",
    "            if ent_type not in unique_entities:\n",
    "                continue\n",
    "            # print(output_entities)\n",
    "            if any(\n",
    "                [\n",
    "                    ent[\"text\"].replace(\" \", \"\")\n",
    "                    == ground_truth_ent[\"text\"].replace(\" \", \"\")\n",
    "                    and ent[\"start\"] == ground_truth_ent[\"start\"]\n",
    "                    for ent in output_entities\n",
    "                ]\n",
    "            ):\n",
    "                tp[ent_type] += 1\n",
    "            else:\n",
    "                fn[ent_type] += 1\n",
    "        for output_ent in output_entities:\n",
    "            ent_type = output_ent[\"entity\"]\n",
    "            if ent_type not in unique_entities:\n",
    "                continue\n",
    "            if output_ent[\"text\"].replace(\" \", \"\") not in ground_truth_texts:\n",
    "                fp[ent_type] += 1\n",
    "    # Calculate precision, recall, and F1 score for each entity\n",
    "    scores = {}\n",
    "    table = []\n",
    "    headers = [\"Entity\", \"Precision\", \"Recall\", \"F1\"]\n",
    "    for entity in unique_entities:\n",
    "        p = (\n",
    "            round(tp[entity] / (tp[entity] + fp[entity]), 2)\n",
    "            if tp[entity] + fp[entity] > 0\n",
    "            else 0\n",
    "        )\n",
    "        r = (\n",
    "            round(tp[entity] / (tp[entity] + fn[entity]), 2)\n",
    "            if tp[entity] + fn[entity] > 0\n",
    "            else 0\n",
    "        )\n",
    "        f1 = round(2 * p * r / (p + r), 2) if p + r > 0 else 0\n",
    "        scores[entity] = {\"precision\": p, \"recall\": r, \"f1\": f1}\n",
    "        table.append([entity, p, r, f1])\n",
    "\n",
    "    general_precision, general_recall, general_f1 = calculate_general_scores(scores)\n",
    "    report = f\"\\nGeneral Precision: {general_precision}\\nGeneral Recall: {general_recall}\\nGeneral F1: {general_f1}\\n{tabulate(table, headers)}\"\n",
    "    with open(output_file, \"w\", encoding=\"ISO-8859-1\") as f:\n",
    "        f.write(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_precision_recall_for_entities(\n",
    "    ground_truth,\n",
    "    prediction_data,\n",
    "    set(\n",
    "        [\"PERSON\", \"EMAIL_ADDRESS\", \"LOCATION\", \"US_SSN\", \"CREDIT_CARD\", \"PHONE_NUMBER\"]\n",
    "    ),\n",
    "    \"eval/REPORT_LG2.txt\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE CASE: BINARY\n",
    "\n",
    "- Given an input text, it indicates whether each entity (person, location, credit_card, phone_number, us_ssn) is present at least once or not.\n",
    "- The use case is applicable for filtering out reviews with sensitive information without needing to know which part of the text has the sensitive information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_entities(ground_truth, output_model, unique_entities, output_file):\n",
    "    # Initialize counters for true positives, false positives, and false negatives\n",
    "    tp = {entity: 0 for entity in unique_entities}\n",
    "    fp = {entity: 0 for entity in unique_entities}\n",
    "    fn = {entity: 0 for entity in unique_entities}\n",
    "\n",
    "    # Loop over the documents in the output model\n",
    "    for i, doc in enumerate(output_model):\n",
    "        # Get the set of entities present in the document\n",
    "        model_entity_set = set()\n",
    "        for entity in doc[\"ENTITIES\"]:\n",
    "            model_entity_set.add(entity[\"entity\"])\n",
    "        # Get the set of entities present in the ground truth in the same index of output model\n",
    "        ground_truth_set = set()\n",
    "        for entity in ground_truth[i][\"ENTITIES\"]:\n",
    "            ground_truth_set.add(entity[\"entity\"])\n",
    "\n",
    "        # Check if each entity in the ground truth is present in the output model\n",
    "        for entity in ground_truth_set:\n",
    "            if entity in model_entity_set:\n",
    "                # Entity is present in both ground truth and output model\n",
    "                tp[entity] += 1\n",
    "            else:\n",
    "                # Entity is present in ground truth but not in output model\n",
    "                fn[entity] += 1\n",
    "\n",
    "        # Check if each entity in the output model is a false positive\n",
    "        for entity in model_entity_set:\n",
    "            if entity not in ground_truth_set:\n",
    "                # Entity is not in the ground truth\n",
    "                fp[entity] += 1\n",
    "\n",
    "    # Calculate precision, recall, and F1 score for each entity\n",
    "    scores = {}\n",
    "    table = []\n",
    "    headers = [\"Entity\", \"Precision\", \"Recall\", \"F1\"]\n",
    "    for entity in unique_entities:\n",
    "        p = (\n",
    "            round(tp[entity] / (tp[entity] + fp[entity]), 2)\n",
    "            if tp[entity] + fp[entity] > 0\n",
    "            else 0\n",
    "        )\n",
    "        r = (\n",
    "            round(tp[entity] / (tp[entity] + fn[entity]), 2)\n",
    "            if tp[entity] + fn[entity] > 0\n",
    "            else 0\n",
    "        )\n",
    "        f1 = round(2 * p * r / (p + r), 2) if p + r > 0 else 0\n",
    "        scores[entity] = {\"precision\": p, \"recall\": r, \"f1\": f1}\n",
    "        table.append([entity, p, r, f1])\n",
    "\n",
    "    general_precision, general_recall, general_f1 = calculate_general_scores(scores)\n",
    "    report = f\"\\nGeneral Precision: {general_precision}\\nGeneral Recall: {general_recall}\\nGeneral F1: {general_f1}\\n{tabulate(table, headers)}\"\n",
    "    with open(output_file, \"w\", encoding=\"ISO-8859-1\") as f:\n",
    "        f.write(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1 = [{'TEXT': 'a1', \"ENTITIES\": [{'start': 21, 'end': 33, 'entity': 'PERSON', 'text': 'Zoey Edwards'}, {'start': 129, 'end': 151, 'entity': 'EMAIL_ADDRESS', 'text': 'edwards-zoey@gmail.com'}, {'start': 190, 'end': 201, 'entity': 'LOCATION', 'text': '900 F St NW'}, {'start': 213, 'end': 222, 'entity': 'US_SSN', 'text': '367245504'}, {'start': 252, 'end': 271, 'entity': 'CREDIT_CARD', 'text': '2259-8740-7030-1462'}]}]\n",
    "# a2 = [{\"TEXT\": \"a2\", \"ENTITIES\": [{'start': 21, 'end': 33, 'entity': 'PERSON', 'text': 'Zoey Edwards'}, {'start': 129, 'end': 151, 'entity': 'EMAIL_ADDRESS', 'text': 'edwards-zoey@gmail.com'}, {'start': 190, 'end': 195, 'entity': 'LOCATION', 'text': '900 F'}, {'start': 196, 'end': 201, 'entity': 'LOCATION', 'text': 'St NW'}, {'start': 213, 'end': 222, 'entity': 'US_SSN', 'text': '367245504'}, {'start': 252, 'end': 255, 'entity': 'PHONE_NUMBER', 'text': '225'}, {'start': 255, 'end': 271, 'entity': 'PHONE_NUMBER', 'text': '9-8740-7030-1462'}]}]\n",
    "evaluate_entities(\n",
    "    ground_truth,\n",
    "    prediction_data,\n",
    "    set(\n",
    "        [\"PERSON\", \"EMAIL_ADDRESS\", \"LOCATION\", \"US_SSN\", \"CREDIT_CARD\", \"PHONE_NUMBER\"]\n",
    "    ),\n",
    "    \"eval/REPORT_SENTENCE_LG.txt\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a9f895cc7e3ca0d623982e8f7234235464dc9c0b943e7d04631c8d05e4aca2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
