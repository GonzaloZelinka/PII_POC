{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTALL DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install presidio_analyzer\n",
    "%pip install presidio_anonymizer\n",
    "%pip install transformers\n",
    "%pip install pandas\n",
    "%pip install spacy\n",
    "%pip install torch\n",
    "%pip install seqeval\n",
    "%pip install spacy-transformers\n",
    "%pip install tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTALL SIMPLE SPACY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTALL COMPLEX SPACY MODEL (ONLY IF YOU USE THIS INSTEAD OF BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gonzalo.zelinka/Desktop/PII_POC/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from presidio_analyzer import AnalyzerEngine, RecognizerResult, RecognizerRegistry\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
    "import pandas as pd\n",
    "from transformers_rec import (\n",
    "    TransformersRecognizer,\n",
    "    BERT_DEID_CONFIGURATION,\n",
    ")\n",
    "import logging\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "from typing import List\n",
    "from spacy import displacy\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from seqeval.metrics import accuracy_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import recall_score\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "import spacy\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE ANALYZER AND ANONYMIZE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzer_engine(model_path):\n",
    "  \"\"\"Return AnalyzerEngine.\n",
    "    :param model_path: Which model to use for NER:\n",
    "        \"obi/deid_roberta_i2b2\",\n",
    "        \"en_core_web_lg\"\n",
    "    \"\"\"\n",
    "  registry = RecognizerRegistry()\n",
    "  registry.load_predefined_recognizers()\n",
    "  if model_path == \"en_core_web_lg\":\n",
    "    nlp_configuration = {\n",
    "        \"nlp_engine_name\": \"spacy\",\n",
    "        \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"}],\n",
    "    }\n",
    "  elif model_path == \"custom_P_E\":\n",
    "    # nlp = spacy.load(\"models/P&E-model3/model-best\")\n",
    "    # registry.add_recognizer(SpacyRecognizer(nlp))\n",
    "    nlp_configuration = {\n",
    "        \"nlp_engine_name\": \"spacy\",\n",
    "        \"models\": [{\"lang_code\": \"en\", \"model_name\": \"models/P&E-model3/model-best\"}],\n",
    "    }\n",
    "  else:\n",
    "    # Using a small spaCy model + a HF NER model\n",
    "    transformers_recognizer = TransformersRecognizer(model_path=model_path)\n",
    "    if model_path == \"obi/deid_roberta_i2b2\":\n",
    "      transformers_recognizer.load_transformer(**BERT_DEID_CONFIGURATION)\n",
    "    # Use small spaCy model, no need for both spacy and HF models\n",
    "    # The transformers model is used here as a recognizer, not as an NlpEngine\n",
    "    nlp_configuration = {\n",
    "      \"nlp_engine_name\": \"spacy\",\n",
    "      \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_sm\"}],\n",
    "    }\n",
    "    registry.add_recognizer(transformers_recognizer)\n",
    "\n",
    "  nlp_engine = NlpEngineProvider(nlp_configuration=nlp_configuration).create_engine()\n",
    "\n",
    "  analyzer = AnalyzerEngine(nlp_engine=nlp_engine, registry=registry)\n",
    "  return analyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(analyzer, **kwargs):\n",
    "    \"\"\"Analyze input using Analyzer engine and input arguments (kwargs).\"\"\"\n",
    "    if \"entities\" not in kwargs or \"All\" in kwargs[\"entities\"]:\n",
    "        kwargs[\"entities\"] = None\n",
    "    return analyzer.analyze(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymize(text: str, analyze_results: List[RecognizerResult]):\n",
    "    \"\"\"Anonymize identified input using Presidio Anonymizer.\n",
    "    :param text: Full text\n",
    "    :param analyze_results: list of results from presidio analyzer engine\n",
    "    \"\"\"\n",
    "    operator_config = {\"lambda\": lambda x: x}\n",
    "    operator = \"custom\"\n",
    "    res = AnonymizerEngine().anonymize(\n",
    "        text,\n",
    "        analyze_results,\n",
    "        operators={\"DEFAULT\": OperatorConfig(operator, operator_config)},\n",
    "    )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIAL CONFIG FOR THE ANALYZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = analyzer_engine(\"obi/deid_roberta_i2b2\") # \"en_core_web_lg\" or \"obi/deid_roberta_i2b2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.40\n",
    "entities = [\"PERSON\", \"LOCATION\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\",\"CREDIT_CARD\", \"US_SSN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_obj(an_r, text):\n",
    "    \"\"\"Show results of analyze() in a dataframe.\"\"\"\n",
    "    ents = []\n",
    "    for r in an_r:\n",
    "      info = r.to_dict()\n",
    "      ent ={ \"start\": info[\"start\"], \n",
    "              \"end\": info['end'], \n",
    "              \"confidence\": info['score'], \n",
    "              \"entity\": info['entity_type'], \n",
    "              \"text\": text[info[\"start\"]:info[\"end\"]]} \n",
    "      ents.append(ent)\n",
    "    return ents\n",
    "\n",
    "\n",
    "def model_results(csv_path, json_path, entities, threshold, analyzer,columns, check_overlaps=False):\n",
    "  results = []\n",
    "  df = pd.read_csv(csv_path, encoding=\"ISO-8859-1\",header=0, names=columns)\n",
    "  # file = open(csv_path, 'r', encoding=\"ISO-8859-1\")\n",
    "  # reader = csv.reader(file)\n",
    "  # rows = list(reader)\n",
    "  # for row in tqdm(rows, total=len(rows)):\n",
    "  for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    # id = row.PVID\n",
    "    # text = row.CONTENT\n",
    "    text = row[0]\n",
    "    analyze_results = analyze(\n",
    "      analyzer=analyzer,\n",
    "      text=text,\n",
    "      entities= entities,\n",
    "      language=\"en\",\n",
    "      score_threshold=threshold,\n",
    "    )\n",
    "    if check_overlaps: # return only entities without overlaps (resolved from presidio) and prediction.\n",
    "      text_anon = anonymize(text, analyze_results)\n",
    "      text_anon = sorted(text_anon.items, key=lambda x: x.start)\n",
    "      result = []\n",
    "      for i, res in enumerate(text_anon):\n",
    "          result.append({\"start\": res.start, \"end\": res.end, \"entity\": res.entity_type, \"text\": res.text})\n",
    "          \n",
    "    else: # return all entities with overlaps and prediction. \n",
    "      result = create_obj(analyze_results, text)\n",
    "    # results.append({\"PVID\": id, \"TEXT\": text, \"ENTITIES\": result})\n",
    "    results.append({\"TEXT\": text, \"ENTITIES\": result})\n",
    "  fp=open(json_path,'w', encoding=\"ISO-8859-1\") # output file\n",
    "  json.dump(results, fp)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST SIMPLE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(text: str, analyze_results: List[RecognizerResult]):\n",
    "    \"\"\"\n",
    "    Highlights every identified entity on top of the text.\n",
    "    :param text: full text\n",
    "    :param analyze_results: list of analyzer results.\n",
    "    \"\"\"\n",
    "    ents = []\n",
    "\n",
    "    # Use the anonymizer to resolve overlaps\n",
    "    results = anonymize(text, analyze_results)\n",
    "    # sort by start index\n",
    "    results = sorted(results.items, key=lambda x: x.start)\n",
    "    for i, res in enumerate(results):\n",
    "        ents.append({\"start\": res.start, \"end\": res.end, \"label\": res.entity_type, \"text\": res.text})\n",
    "    return [{\"text\": text, \"ents\": ents}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(an_r, text, return_analyzer_results=False):\n",
    "    \"\"\"Show results of analyze() in a dataframe.\"\"\"\n",
    "    df = pd.DataFrame.from_records([r.to_dict() for r in an_r])\n",
    "    df[\"text\"] = [text[res.start: res.end] for res in an_r]\n",
    "    df_subset = df[[\"entity_type\", \"text\", \"start\", \"end\", \"score\"]].rename(\n",
    "        {\n",
    "            \"entity_type\": \"Entity type\",\n",
    "            \"text\": \"Text\",\n",
    "            \"start\": \"Start\",\n",
    "            \"end\": \"End\",\n",
    "            \"score\": \"Confidence\",\n",
    "        },\n",
    "        axis=1,\n",
    "    )\n",
    "    df_subset[\"Text\"] = [text[res.start: res.end] for res in an_r]\n",
    "    #  In analysis_explanation_df there are more columns than in df_subset with more information. \n",
    "    if return_analyzer_results:\n",
    "      analysis_explanation_df = pd.DataFrame.from_records(\n",
    "          [r.analysis_explanation.to_dict() for r in an_r]\n",
    "      )\n",
    "    # df_subset = pd.concat([df_subset, analysis_explanation_df], axis=1)\n",
    "    result = annotate(text, an_r)\n",
    "    return df_subset.reset_index(drop=True), result\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"My name is Gonzalo Zelinka, I'm from Argentina and I live in Barcelona. My phone number is +34 666 666 666 and my email is gonzalozelinka@gmail.com. I work at Microsoft and my credit card number is 1234 5678 9012 3456. My SSN is 123-45-6789.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_results = analyze(\n",
    "    analyzer=analyzer,\n",
    "    text=text,\n",
    "    entities= entities,\n",
    "    language=\"en\",\n",
    "    score_threshold=threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">My name is \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Gonzalo Zelinka\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", I'm from \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Argentina\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOCATION</span>\n",
       "</mark>\n",
       " and I live in \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Barcelona\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOCATION</span>\n",
       "</mark>\n",
       ". My phone number is \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    +34 666 666 666\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUMBER</span>\n",
       "</mark>\n",
       " and my email is \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    gonzalozelinka@gmail.com\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EMAIL_ADDRESS</span>\n",
       "</mark>\n",
       ". I work at Microsoft and my credit card number is \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    12\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUMBER</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    34 5678 9012\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUMBER</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    3456\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUMBER</span>\n",
       "</mark>\n",
       ". My SSN is \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    123\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    -45-6789\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUMBER</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity type</th>\n",
       "      <th>Text</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PERSON</td>\n",
       "      <td>Zelinka</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LOCATION</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>37</td>\n",
       "      <td>46</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOCATION</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>61</td>\n",
       "      <td>70</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EMAIL_ADDRESS</td>\n",
       "      <td>gonzalozelinka@gmail.com</td>\n",
       "      <td>123</td>\n",
       "      <td>147</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PERSON</td>\n",
       "      <td>Gonzalo</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PHONE_NUMBER</td>\n",
       "      <td>666</td>\n",
       "      <td>103</td>\n",
       "      <td>106</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PHONE_NUMBER</td>\n",
       "      <td>3456</td>\n",
       "      <td>213</td>\n",
       "      <td>217</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PHONE_NUMBER</td>\n",
       "      <td>+34 666 666</td>\n",
       "      <td>91</td>\n",
       "      <td>102</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PHONE_NUMBER</td>\n",
       "      <td>12</td>\n",
       "      <td>198</td>\n",
       "      <td>200</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>O</td>\n",
       "      <td>123</td>\n",
       "      <td>229</td>\n",
       "      <td>232</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PHONE_NUMBER</td>\n",
       "      <td>34 5678 9012</td>\n",
       "      <td>200</td>\n",
       "      <td>212</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PERSON</td>\n",
       "      <td>Gonzalo Zelinka</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>EMAIL</td>\n",
       "      <td>gonzalo</td>\n",
       "      <td>123</td>\n",
       "      <td>130</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PHONE_NUMBER</td>\n",
       "      <td>-45-6789</td>\n",
       "      <td>232</td>\n",
       "      <td>240</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PHONE_NUMBER</td>\n",
       "      <td>+34 666 666 666</td>\n",
       "      <td>91</td>\n",
       "      <td>106</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PERSON</td>\n",
       "      <td>inka</td>\n",
       "      <td>133</td>\n",
       "      <td>137</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PERSON</td>\n",
       "      <td>zel</td>\n",
       "      <td>130</td>\n",
       "      <td>133</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Entity type                      Text  Start  End  Confidence\n",
       "0          PERSON                   Zelinka     19   26        1.00\n",
       "1        LOCATION                 Argentina     37   46        1.00\n",
       "2        LOCATION                 Barcelona     61   70        1.00\n",
       "3   EMAIL_ADDRESS  gonzalozelinka@gmail.com    123  147        1.00\n",
       "4          PERSON                   Gonzalo     11   18        0.99\n",
       "5    PHONE_NUMBER                       666    103  106        0.99\n",
       "6    PHONE_NUMBER                      3456    213  217        0.98\n",
       "7    PHONE_NUMBER               +34 666 666     91  102        0.97\n",
       "8    PHONE_NUMBER                        12    198  200        0.93\n",
       "9               O                       123    229  232        0.93\n",
       "10   PHONE_NUMBER              34 5678 9012    200  212        0.92\n",
       "11         PERSON           Gonzalo Zelinka     11   26        0.85\n",
       "12          EMAIL                   gonzalo    123  130        0.82\n",
       "13   PHONE_NUMBER                  -45-6789    232  240        0.81\n",
       "14   PHONE_NUMBER           +34 666 666 666     91  106        0.75\n",
       "15         PERSON                      inka    133  137        0.56\n",
       "16         PERSON                       zel    130  133        0.55"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frame, sentence = show_results(analyze_results, text)\n",
    "# print(sentence)\n",
    "displacy.render(sentence, style=\"ent\", manual=True)\n",
    "display(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERATE RESULTS FROM COMPLEX DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHANGE DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "#Change this\n",
    "os.chdir(\"\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results(\"testing-data/product_reviews7_test_sentences.csv\", \"testing-data/product_reviews8.json\", entities, threshold, analyzer, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT THE GROUND TRUTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span_indx(\n",
    "    labels: List[str],\n",
    "    words: List[str],\n",
    "    sentence: str\n",
    ") -> List[tuple]:\n",
    "    \"\"\"Gets span starts and ends for Spacy spancat component.\n",
    "        \n",
    "        Returns list of tuples where the first element of the \n",
    "        tuple is the span start, the second element of the tuple\n",
    "        is the span end and the third element of the tuple is\n",
    "        the span category. \n",
    "    \"\"\"\n",
    "    #gets list of indices corresponding to labelled words \n",
    "    label_indx = []\n",
    "    temp_list = []\n",
    "\n",
    "    for i, l in enumerate(labels):\n",
    "        if l != 'O':\n",
    "            temp_list.append(i)\n",
    "        else:\n",
    "            label_indx.append(temp_list)\n",
    "            temp_list = []    \n",
    "        if i == len(labels) - 1:\n",
    "            label_indx.append(temp_list)\n",
    "\n",
    "    clean_label_indx = [x for x in label_indx if len(x) > 0]\n",
    "\n",
    "    spans = []\n",
    "    for indx in clean_label_indx:\n",
    "        if len(indx) == 1:\n",
    "            span = words[indx[0]]\n",
    "            label = labels[indx[0]].upper()\n",
    "        else:\n",
    "            span = ' '.join([words[i] for i in indx])  \n",
    "            label = [labels[i].upper() for i in indx][0]\n",
    "        #remove punctuation and strip whitespace for spans\n",
    "        span_clean = span.strip()\n",
    "        for m in re.finditer(re.escape(span_clean), sentence):\n",
    "            spans.append({\"start\":m.start(), \"end\":m.end(), \"entity\": label, \"text\": m.group()})\n",
    "    \n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_csv_annotated_to_json(input_path):\n",
    "    DATA = []\n",
    "    data = (pd.read_csv(input_path, encoding='ISO-8859-1')\n",
    "          .fillna(method='ffill'))\n",
    "    for sent, sent_info in data.groupby('Review #'):\n",
    "      words = list(sent_info[\"Word\"])\n",
    "      #convert words to sentence and get rid of spaces between punctuation characters\n",
    "      sentence = re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', \" \".join(words))\n",
    "      #get labels\n",
    "      labels = list(sent_info['Tag'])\n",
    "      #identify token span start, span ends and span category\n",
    "      span_ents = get_span_indx(labels, words, sentence)\n",
    "      DATA.append({\"TEXT\": sentence, \"ENTITIES\": span_ents})\n",
    "    return DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TEXT': 'Hi my name is Sofia thompson. I hate the Xiaomi , the design is awful. I expected more. My social sn is 068289873 and my credit number is 4881 8730 3709 2414', 'ENTITIES': [{'start': 14, 'end': 28, 'entity': 'PERSON', 'text': 'Sofia thompson'}, {'start': 104, 'end': 113, 'entity': 'US_SSN', 'text': '068289873'}, {'start': 138, 'end': 157, 'entity': 'CREDIT_CARD', 'text': '4881 8730 3709 2414'}]}\n"
     ]
    }
   ],
   "source": [
    "ground_truth = transform_csv_annotated_to_json(\"testing-data/product_reviews9.csv\")\n",
    "with open('testing-data/true_data.json', 'w') as fp:\n",
    "    json.dump(ground_truth, fp)\n",
    "print(ground_truth[1999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT ONLY SENTENCES TO SEND TO THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(input_file, output_file):\n",
    "  data = (pd.read_csv(input_file, encoding='ISO-8859-1')\n",
    "    .fillna(method='ffill'))\n",
    "  with open(output_file, 'w', encoding='ISO-8859-1') as fo:\n",
    "    writer = csv.writer(fo)\n",
    "    writer.writerow(['SENTENCES']) \n",
    "    for sent, sent_info in data.groupby('Review #'):\n",
    "      words = list(sent_info[\"Word\"])\n",
    "      sentence = re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', \" \".join(words))\n",
    "      writer.writerow([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_sentences(\"testing-data/product_reviews9.csv\", \"testing-data/sentences_evaluate.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN THE MODEL WITH THE EXTRACTED SENTENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:02<00:00,  6.61it/s]\n"
     ]
    }
   ],
   "source": [
    "model_results(csv_path=\"testing-data/sentences_evaluate.csv\", json_path=\"testing-data/output_roberta3.json\", \n",
    "entities=entities, threshold=threshold, analyzer=analyzer, columns=[\"SENTENCES\"], check_overlaps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REMOVE UNWANTED ENTITIES FROM THE MODEL RESULTS AND JOIN ENTITIES SUBDIVIDED INTO ONE\n",
    "- Sometimes the model divide entities into several entities, so we need to join them to evaluate the results. For example, if the model found the entity \"686 E Broadway\" as \"6\", \"86 E\" and \"Broadway\", we need to join them to evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_entities_to_eval(input_path, output_path):\n",
    "    # Read the input JSON file\n",
    "    with open(input_path, \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "        json_data = f.read()\n",
    "    # Load the JSON data into a Python list\n",
    "    data_to_check = json.loads(json_data)\n",
    "    # Loop over each item in the list\n",
    "    for js in data_to_check:\n",
    "        # Remove entities with \"O\" entity value\n",
    "        js[\"ENTITIES\"] = [ent for ent in js[\"ENTITIES\"] if ent[\"entity\"] != \"O\"]\n",
    "\n",
    "        # Fix overlapping entities and combine entities with a single character distance\n",
    "        entities_clean = []\n",
    "        i = 0 \n",
    "        # Loop over each entity in the current item\n",
    "        # print(\"js[ENTITIES]: \", js[\"ENTITIES\"])\n",
    "        while i < len(js[\"ENTITIES\"]):\n",
    "            j = i + 1\n",
    "            new_entity = js[\"ENTITIES\"][i]\n",
    "            # Combine adjacent entities that have the same type and are next to each other\n",
    "            while j < len(js[\"ENTITIES\"]):\n",
    "                if (js[\"ENTITIES\"][j]['entity'] != js[\"ENTITIES\"][i]['entity'] \n",
    "                    or int(js[\"ENTITIES\"][j][\"start\"]) - int(js[\"ENTITIES\"][i][\"end\"]) > 1):\n",
    "                    # If the next entity is not the same type or is not adjacent, stop combining entities\n",
    "                    break\n",
    "                new_entity[\"end\"] = int(js[\"ENTITIES\"][j][\"end\"])\n",
    "                # print(\"new_entity end: \", new_entity[\"end\"])\n",
    "                new_entity[\"text\"] = new_entity[\"text\"] + \" \" + js[\"ENTITIES\"][j][\"text\"]\n",
    "                # print(\"new_entity text: \", new_entity[\"text\"])\n",
    "                j += 1\n",
    "\n",
    "            entities_clean.append(new_entity)\n",
    "            i = j\n",
    "        # Update the current item with the cleaned entities\n",
    "        # print(\"entities_clean final: \", entities_clean)\n",
    "        js[\"ENTITIES\"] = entities_clean\n",
    "    # Write the updated JSON data to the output file\n",
    "    with open(output_path, 'w', encoding=\"ISO-8859-1\") as fp:\n",
    "        json.dump(data_to_check, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_entities_to_eval(\"testing-data/output_lg.json\", \"testing-data/output_lg_fixed.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testing-data/output_lg_fixed.json\", \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "  json_data = f.read()\n",
    "prediction_data = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TEXT': \"Hey there I'm Hannah Anderson. I regret buying the OnePlus , the screen resolution seems to be poor. You can email me at anderson@gmail.com or text me at +06574316875. Disappointing and frustrating. I am located at 686 E Broadway. My card is 2299986383983760\",\n",
       " 'ENTITIES': [{'start': 14,\n",
       "   'end': 29,\n",
       "   'entity': 'PERSON',\n",
       "   'text': 'Hannah Anderson'},\n",
       "  {'start': 121,\n",
       "   'end': 139,\n",
       "   'entity': 'EMAIL_ADDRESS',\n",
       "   'text': 'anderson@gmail.com'}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_general_scores(entity_scores):\n",
    "    precision_sum = 0\n",
    "    recall_sum = 0\n",
    "    f1_sum = 0\n",
    "    for entity, scores in entity_scores.items():\n",
    "        precision_sum += scores['precision']\n",
    "        recall_sum += scores['recall']\n",
    "        f1_sum += scores['f1']\n",
    "\n",
    "    num_entities = len(entity_scores)\n",
    "    general_precision = precision_sum / num_entities\n",
    "    general_recall = recall_sum / num_entities\n",
    "    general_f1 = f1_sum / num_entities\n",
    "\n",
    "    return general_precision, general_recall, general_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE CASE: EXACT\n",
    "- Identifies the exact words associated with all PII entities in the input text.\n",
    "- This use case is applicable if the client wants to know which exact words correspond to the PII information. For example to apply masks over the PII entities detected in the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall_for_entities(ground_truth, output_model, unique_entities, output_file):\n",
    "    # Initialize counters for true positives, false positives, and false negatives\n",
    "    tp = {entity: 0 for entity in unique_entities}\n",
    "    fp = {entity: 0 for entity in unique_entities}\n",
    "    fn = {entity: 0 for entity in unique_entities}\n",
    "\n",
    "    for example_idx in range(len(ground_truth)):\n",
    "        ground_truth_entities = ground_truth[example_idx]['ENTITIES']\n",
    "        output_entities = output_model[example_idx]['ENTITIES']\n",
    "        # create a set of output entity texts for quick lookup\n",
    "        ground_truth_texts = set([ent['text'].replace(\" \", \"\") for ent in ground_truth_entities])\n",
    "\n",
    "        for ground_truth_ent in ground_truth_entities:\n",
    "            ent_type = ground_truth_ent['entity']\n",
    "            if ent_type not in unique_entities:\n",
    "                continue\n",
    "            # print(output_entities)\n",
    "            if any([ent['text'].replace(\" \", \"\") == ground_truth_ent['text'].replace(\" \",\"\") and ent[\"start\"] == ground_truth_ent[\"start\"] for ent in output_entities]):\n",
    "                tp[ent_type] += 1\n",
    "            else:\n",
    "                fn[ent_type] += 1\n",
    "        for output_ent in output_entities:\n",
    "            ent_type = output_ent['entity']\n",
    "            if ent_type not in unique_entities:\n",
    "                continue\n",
    "            if output_ent['text'].replace(\" \",\"\") not in ground_truth_texts:\n",
    "                fp[ent_type] += 1\n",
    "    # Calculate precision, recall, and F1 score for each entity\n",
    "    scores = {}\n",
    "    table = []\n",
    "    headers = ['Entity', 'Precision', 'Recall', 'F1']\n",
    "    for entity in unique_entities:\n",
    "        p = round(tp[entity] / (tp[entity] + fp[entity]), 2) if tp[entity] + fp[entity] > 0 else 0\n",
    "        r = round(tp[entity] / (tp[entity] + fn[entity]), 2) if tp[entity] + fn[entity] > 0 else 0\n",
    "        f1 = round(2 * p * r / (p + r), 2) if p + r > 0 else 0\n",
    "        scores[entity] = {'precision': p, 'recall': r, 'f1': f1}\n",
    "        table.append([entity, p, r, f1])\n",
    "    \n",
    "    general_precision, general_recall, general_f1 = calculate_general_scores(scores)\n",
    "    report = f\"\\nGeneral Precision: {general_precision}\\nGeneral Recall: {general_recall}\\nGeneral F1: {general_f1}\\n{tabulate(table, headers)}\"\n",
    "    with open(output_file, 'w', encoding='ISO-8859-1') as f:\n",
    "        f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_precision_recall_for_entities(ground_truth, prediction_data,set([\"PERSON\", \"EMAIL_ADDRESS\", \"LOCATION\", \"US_SSN\", \"CREDIT_CARD\", \"PHONE_NUMBER\"]) , \"eval/REPORT_LG2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE CASE: BINARY\n",
    "- Given an input text, it indicates whether each entity (person, location, credit_card, phone_number, us_ssn) is present at least once or not.\n",
    "- The use case is applicable for filtering out reviews with sensitive information without needing to know which part of the text has the sensitive information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_entities(ground_truth, output_model, unique_entities, output_file):\n",
    "    # Initialize counters for true positives, false positives, and false negatives\n",
    "    tp = {entity: 0 for entity in unique_entities}\n",
    "    fp = {entity: 0 for entity in unique_entities}\n",
    "    fn = {entity: 0 for entity in unique_entities}\n",
    "    \n",
    "    # Loop over the documents in the output model\n",
    "    for i, doc in enumerate(output_model):\n",
    "        # Get the set of entities present in the document\n",
    "        model_entity_set = set()\n",
    "        for entity in doc['ENTITIES']:\n",
    "            model_entity_set.add(entity['entity'])\n",
    "        # Get the set of entities present in the ground truth in the same index of output model\n",
    "        ground_truth_set = set()\n",
    "        for entity in ground_truth[i]['ENTITIES']:\n",
    "            ground_truth_set.add(entity['entity'])\n",
    "            \n",
    "        # Check if each entity in the ground truth is present in the output model\n",
    "        for entity in ground_truth_set:\n",
    "            if entity in model_entity_set:\n",
    "                # Entity is present in both ground truth and output model\n",
    "                tp[entity] += 1\n",
    "            else:\n",
    "                # Entity is present in ground truth but not in output model\n",
    "                fn[entity] += 1\n",
    "        \n",
    "        # Check if each entity in the output model is a false positive\n",
    "        for entity in model_entity_set:\n",
    "            if entity not in ground_truth_set:\n",
    "                # Entity is not in the ground truth\n",
    "                fp[entity] += 1\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score for each entity\n",
    "    scores = {}\n",
    "    table = []\n",
    "    headers = ['Entity', 'Precision', 'Recall', 'F1']\n",
    "    for entity in unique_entities:\n",
    "        p = round(tp[entity] / (tp[entity] + fp[entity]), 2) if tp[entity] + fp[entity] > 0 else 0\n",
    "        r = round(tp[entity] / (tp[entity] + fn[entity]), 2) if tp[entity] + fn[entity] > 0 else 0\n",
    "        f1 = round(2 * p * r / (p + r), 2) if p + r > 0 else 0\n",
    "        scores[entity] = {'precision': p, 'recall': r, 'f1': f1}\n",
    "        table.append([entity, p, r, f1])\n",
    "    \n",
    "    general_precision, general_recall, general_f1 = calculate_general_scores(scores)\n",
    "    report = f\"\\nGeneral Precision: {general_precision}\\nGeneral Recall: {general_recall}\\nGeneral F1: {general_f1}\\n{tabulate(table, headers)}\"\n",
    "    with open(output_file, 'w', encoding='ISO-8859-1') as f:\n",
    "        f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1 = [{'TEXT': 'a1', \"ENTITIES\": [{'start': 21, 'end': 33, 'entity': 'PERSON', 'text': 'Zoey Edwards'}, {'start': 129, 'end': 151, 'entity': 'EMAIL_ADDRESS', 'text': 'edwards-zoey@gmail.com'}, {'start': 190, 'end': 201, 'entity': 'LOCATION', 'text': '900 F St NW'}, {'start': 213, 'end': 222, 'entity': 'US_SSN', 'text': '367245504'}, {'start': 252, 'end': 271, 'entity': 'CREDIT_CARD', 'text': '2259-8740-7030-1462'}]}]\n",
    "# a2 = [{\"TEXT\": \"a2\", \"ENTITIES\": [{'start': 21, 'end': 33, 'entity': 'PERSON', 'text': 'Zoey Edwards'}, {'start': 129, 'end': 151, 'entity': 'EMAIL_ADDRESS', 'text': 'edwards-zoey@gmail.com'}, {'start': 190, 'end': 195, 'entity': 'LOCATION', 'text': '900 F'}, {'start': 196, 'end': 201, 'entity': 'LOCATION', 'text': 'St NW'}, {'start': 213, 'end': 222, 'entity': 'US_SSN', 'text': '367245504'}, {'start': 252, 'end': 255, 'entity': 'PHONE_NUMBER', 'text': '225'}, {'start': 255, 'end': 271, 'entity': 'PHONE_NUMBER', 'text': '9-8740-7030-1462'}]}]\n",
    "evaluate_entities(ground_truth, prediction_data, set([\"PERSON\", \"EMAIL_ADDRESS\", \"LOCATION\", \"US_SSN\", \"CREDIT_CARD\", \"PHONE_NUMBER\"]), \"eval/REPORT_SENTENCE_LG.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a9f895cc7e3ca0d623982e8f7234235464dc9c0b943e7d04631c8d05e4aca2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
