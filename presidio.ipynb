{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTALL DEPENDENCIES\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR MAC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR CUDA (GPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install presidio_analyzer\n",
    "%pip install presidio_anonymizer\n",
    "%pip install transformers\n",
    "%pip install pandas\n",
    "%pip install spacy\n",
    "%pip install spacy-transformers\n",
    "%pip install tabulate\n",
    "%pip install multiprocess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTALL SIMPLE SPACY MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTALL COMPLEX SPACY MODEL (ONLY IF YOU USE THIS INSTEAD OF BERT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\GIZ_C\\Desktop\\PII_POC\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'fix_entities_to_eval' from 'utils.fix_entities_model' (g:\\GIZ_C\\Desktop\\PII_POC\\utils\\fix_entities_model.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers_rec\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     BERT_DEID_CONFIGURATION,\n\u001b[0;32m     19\u001b[0m     Analyzer,\n\u001b[0;32m     20\u001b[0m     Anonymizer,\n\u001b[0;32m     21\u001b[0m     TransformersRecognizer,\n\u001b[0;32m     22\u001b[0m )\n\u001b[1;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m fix_entities_to_eval, model_results, transform_csv_annotated_to_json\n",
      "File \u001b[1;32mg:\\GIZ_C\\Desktop\\PII_POC\\utils\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mextract_gt_to_evaluate\u001b[39;00m \u001b[39mimport\u001b[39;00m transform_csv_annotated_to_json\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfix_entities_model\u001b[39;00m \u001b[39mimport\u001b[39;00m fix_entities_to_eval\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mrun_model\u001b[39;00m \u001b[39mimport\u001b[39;00m model_results\n\u001b[0;32m      5\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mtransform_csv_annotated_to_json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel_results\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfix_entities_to_eval\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'fix_entities_to_eval' from 'utils.fix_entities_model' (g:\\GIZ_C\\Desktop\\PII_POC\\utils\\fix_entities_model.py)"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "from presidio_analyzer import RecognizerResult\n",
    "from spacy import displacy\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "from transformers_rec import (\n",
    "    Analyzer,\n",
    "    Anonymizer,\n",
    ")\n",
    "from utils import fix_entities_to_eval, model_results, transform_csv_annotated_to_json\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING CUDA FOR GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIAL CONFIG FOR THE ANALYZER AND MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = Analyzer(\n",
    "    \"obi/deid_roberta_i2b2\", type=\"simple\"\n",
    ")  # \"en_core_web_lg\" or \"obi/deid_roberta_i2b2\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST SIMPLE DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(text: str, analyze_results: List[RecognizerResult]):\n",
    "    \"\"\"\n",
    "    Highlights every identified entity on top of the text.\n",
    "    :param text: full text\n",
    "    :param analyze_results: list of analyzer results.\n",
    "    \"\"\"\n",
    "    ents = []\n",
    "\n",
    "    # Use the anonymizer to resolve overlaps\n",
    "    anonymizer = Anonymizer()\n",
    "    results = anonymizer.anonymize(text, analyze_results)\n",
    "    # sort by start index\n",
    "    results = sorted(results.items, key=lambda x: x.start)\n",
    "    for i, res in enumerate(results):\n",
    "        ents.append(\n",
    "            {\n",
    "                \"start\": res.start,\n",
    "                \"end\": res.end,\n",
    "                \"label\": res.entity_type,\n",
    "                \"text\": res.text,\n",
    "            }\n",
    "        )\n",
    "    return [{\"text\": text, \"ents\": ents}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(an_r, text, return_analyzer_results=False):\n",
    "    \"\"\"Show results of analyze() in a dataframe.\"\"\"\n",
    "    df = pd.DataFrame.from_records([r.to_dict() for r in an_r])\n",
    "    df[\"text\"] = [text[res.start : res.end] for res in an_r]\n",
    "    df_subset = df[[\"entity_type\", \"text\", \"start\", \"end\", \"score\"]].rename(\n",
    "        {\n",
    "            \"entity_type\": \"Entity type\",\n",
    "            \"text\": \"Text\",\n",
    "            \"start\": \"Start\",\n",
    "            \"end\": \"End\",\n",
    "            \"score\": \"Confidence\",\n",
    "        },\n",
    "        axis=1,\n",
    "    )\n",
    "    df_subset[\"Text\"] = [text[res.start : res.end] for res in an_r]\n",
    "    #  In analysis_explanation_df there are more columns than in df_subset with more information.\n",
    "    if return_analyzer_results:\n",
    "        analysis_explanation_df = pd.DataFrame.from_records(\n",
    "            [r.analysis_explanation.to_dict() for r in an_r]\n",
    "        )\n",
    "    # df_subset = pd.concat([df_subset, analysis_explanation_df], axis=1)\n",
    "    result = annotate(text, an_r)\n",
    "    return df_subset.reset_index(drop=True), result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I got these to match my VIVID TIGER HOODIE the combo was perfect and I got complements will recomend.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_results = analyzer.analyze(\n",
    "    text=text,\n",
    "    entities=[\n",
    "        \"PERSON\",\n",
    "        \"LOCATION\",\n",
    "        \"PHONE_NUMBER\",\n",
    "        \"EMAIL_ADDRESS\",\n",
    "        \"CREDIT_CARD\",\n",
    "        \"US_SSN\",\n",
    "    ],\n",
    "    language=\"en\",\n",
    "    score_threshold={\n",
    "        \"PERSON\": 0.9,\n",
    "        \"CREDIT_CARD\": 0.5,\n",
    "        \"US_SSN\": 0.5,\n",
    "        \"EMAIL_ADDRESS\": 0.5,\n",
    "        \"PHONE_NUMBER\": 0.4,\n",
    "        \"LOCATION\": 0.7,\n",
    "    },\n",
    ")\n",
    "print(analyze_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_batch = Analyzer(\"obi/deid_roberta_i2b2\", type=\"batch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"I got these to match my VIVID TIGER HOODIE the combo was perfect and I got complements will recomend.\",\n",
    "    \"As usual, my experience with Mission Belt was just fine. If you need more help, text me at 555-3424-5324\",\n",
    "    \"As a customer for 5+ years, we know that a Mission Belt is a great gift, too. Our friend had never seen anything like it. My name is Gonzalo Zelinka\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[type: PERSON, start: 24, end: 25, score: 0.949999988079071], [type: PHONE_NUMBER, start: 91, end: 94, score: 1.0, type: PHONE_NUMBER, start: 94, end: 104, score: 0.8100000023841858], [type: PERSON, start: 141, end: 148, score: 0.9800000190734863, type: PERSON, start: 133, end: 140, score: 0.9700000286102295, type: LOCATION, start: 133, end: 148, score: 0.85]]\n"
     ]
    }
   ],
   "source": [
    "analyze_results = analyze_batch.analyze(\n",
    "    texts=texts,\n",
    "    language=\"en\",\n",
    "    entities=[\n",
    "        \"PERSON\",\n",
    "        \"LOCATION\",\n",
    "        \"PHONE_NUMBER\",\n",
    "        \"EMAIL_ADDRESS\",\n",
    "        \"CREDIT_CARD\",\n",
    "        \"US_SSN\",\n",
    "    ],\n",
    "    score_threshold={\n",
    "        \"PERSON\": 0.9,\n",
    "        \"CREDIT_CARD\": 0.5,\n",
    "        \"US_SSN\": 0.5,\n",
    "        \"EMAIL_ADDRESS\": 0.5,\n",
    "        \"PHONE_NUMBER\": 0.4,\n",
    "        \"LOCATION\": 0.7,\n",
    "    },\n",
    ")\n",
    "print(analyze_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(analyze_results) > 0:\n",
    "    frame, sentence = show_results(analyze_results, text)\n",
    "    # print(sentence)\n",
    "    displacy.render(sentence, style=\"ent\", manual=True)\n",
    "    display(frame)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACT SENTENCES TO SEND THE MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_to_eval(input_file, output_file, is_list_reviews=True):\n",
    "    data = pd.read_csv(input_file, encoding=\"ISO-8859-1\").fillna(method=\"ffill\")\n",
    "    with open(output_file, \"w\", encoding=\"ISO-8859-1\") as fo:\n",
    "        writer = csv.writer(fo)\n",
    "        num_reviews = 0\n",
    "        for index, row in data.iterrows():\n",
    "            if is_list_reviews:\n",
    "                reviews_list = eval(row[\"REVIEWS\"])\n",
    "                for review in reviews_list:\n",
    "                    num_reviews = num_reviews + 1\n",
    "                    new_row = [\n",
    "                        str(row[\"SITE_URL\"]),\n",
    "                        row[\"ITEM_GROUP_ID\"],\n",
    "                        row[\"PVIDS\"],\n",
    "                        re.sub(r\"[\\n\\r]{1,}\", \" \", str(review)),\n",
    "                    ]\n",
    "                    writer.writerow(new_row)\n",
    "            else:\n",
    "                num_reviews = num_reviews + 1\n",
    "                new_row = [\n",
    "                    str(row[\"SITE_URL\"]),\n",
    "                    row[\"ITEM_GROUP_ID\"],\n",
    "                    re.sub(r\"[\\n\\r]{1,}\", \" \", row[\"PVIDS\"]),\n",
    "                    re.sub(r\"[\\n\\r]{1,}\", \" \", str(row[\"REVIEWS\"])),\n",
    "                ]\n",
    "                writer.writerow(new_row)\n",
    "        return num_reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 112055\n"
     ]
    }
   ],
   "source": [
    "num_reviews = extract_sentences_to_eval(\n",
    "    \"testing-data/reviews_from_10_sites.csv\",\n",
    "    \"testing-data/cleaned_sentences_from_db3.csv\",\n",
    "    is_list_reviews=False,\n",
    ")\n",
    "print(f\"Number of reviews: {num_reviews}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because we use n-calls, the warning is annoying as it repeats itself all the time\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\"You seem to be using the pipelines sequentially on GPU.\"\n",
    ")\n",
    "model_results(\n",
    "    input_path=\"testing-data/cleaned_sentences_from_db2.csv\",\n",
    "    output_path=\"testing-data/output_from_db3.csv\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATE MODEL\n",
    "\n",
    "The format to the dataset if like this:\n",
    "\n",
    "- [Example Dataset](https://www.kaggle.com/datasets/namanj27/ner-dataset)\n",
    "\n",
    "Column necessaries and their names:\n",
    "\n",
    "- Sentence # --> Review #\n",
    "- Word --> Word\n",
    "- POS --> Delete this.\n",
    "- Tag --> Tag.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT THE GROUND TRUTH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = transform_csv_annotated_to_json(\"testing-data/product_reviews9.csv\")\n",
    "print(ground_truth[1999])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT ONLY SENTENCES TO SEND TO THE MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_to_eval(input_file, output_file):\n",
    "    data = pd.read_csv(input_file, encoding=\"ISO-8859-1\").fillna(method=\"ffill\")\n",
    "    with open(output_file, \"w\", encoding=\"ISO-8859-1\") as fo:\n",
    "        writer = csv.writer(fo)\n",
    "        writer.writerow([\"SENTENCES\"])\n",
    "        for sent, sent_info in data.groupby(\"Review #\"):\n",
    "            words = list(sent_info[\"Word\"])\n",
    "            sentence = re.sub(r'\\s([?.!\"](?:\\s|$))', r\"\\1\", \" \".join(words))\n",
    "            writer.writerow([sentence])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_sentences_to_eval(\n",
    "    \"testing-data/product_reviews9.csv\", \"testing-data/sentences_evaluate.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_from_db(input_file, output_file):\n",
    "    data = pd.read_csv(input_file, encoding=\"ISO-8859-1\").fillna(method=\"ffill\")\n",
    "    reviews = []\n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        for review in json.loads(row[\"REVIEWS\"]):\n",
    "            reviews.append([row[\"SITE_URL\"], row[\"PVID\"], review])\n",
    "    new_data = pd.DataFrame(reviews, columns=[\"SITE_URL\", \"PVID\", \"REVIEW\"])\n",
    "    new_data.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_sentences_from_db(\n",
    "    \"testing-data/reviews.csv\", \"testing-data/sentences_from_db2.csv\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN THE MODEL WITH THE EXTRACTED SENTENCES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results(\n",
    "    input_path=\"testing-data/input-path.csv\",\n",
    "    output_path=\"testing-data/output-path.json\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REMOVE UNWANTED ENTITIES FROM THE MODEL RESULTS AND JOIN ENTITIES SUBDIVIDED INTO ONE\n",
    "\n",
    "- Sometimes the model divide entities into several entities, so we need to join them to evaluate the results. For example, if the model found the entity \"686 E Broadway\" as \"6\", \"86 E\" and \"Broadway\", we need to join them to evaluate the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_entities_to_eval(\"testing-data/input_path.json\", \"testing-data/output_path.json\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testing-data/output_path.json\", \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "    json_data = f.read()\n",
    "prediction_data = json.loads(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_general_scores(entity_scores):\n",
    "    precision_sum = 0\n",
    "    recall_sum = 0\n",
    "    f1_sum = 0\n",
    "    for entity, scores in entity_scores.items():\n",
    "        precision_sum += scores[\"precision\"]\n",
    "        recall_sum += scores[\"recall\"]\n",
    "        f1_sum += scores[\"f1\"]\n",
    "\n",
    "    num_entities = len(entity_scores)\n",
    "    general_precision = precision_sum / num_entities\n",
    "    general_recall = recall_sum / num_entities\n",
    "    general_f1 = f1_sum / num_entities\n",
    "\n",
    "    return general_precision, general_recall, general_f1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE CASE: EXACT\n",
    "\n",
    "- Identifies the exact words associated with all PII entities in the input text.\n",
    "- This use case is applicable if the client wants to know which exact words correspond to the PII information. For example to apply masks over the PII entities detected in the input text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall_for_entities(\n",
    "    ground_truth, output_model, unique_entities, output_file\n",
    "):\n",
    "    # Initialize counters for true positives, false positives, and false negatives\n",
    "    tp = {entity: 0 for entity in unique_entities}\n",
    "    fp = {entity: 0 for entity in unique_entities}\n",
    "    fn = {entity: 0 for entity in unique_entities}\n",
    "\n",
    "    for example_idx in range(len(ground_truth)):\n",
    "        ground_truth_entities = ground_truth[example_idx][\"ENTITIES\"]\n",
    "        output_entities = output_model[example_idx][\"ENTITIES\"]\n",
    "        # create a set of output entity texts for quick lookup\n",
    "        ground_truth_texts = set(\n",
    "            [ent[\"text\"].replace(\" \", \"\") for ent in ground_truth_entities]\n",
    "        )\n",
    "\n",
    "        for ground_truth_ent in ground_truth_entities:\n",
    "            ent_type = ground_truth_ent[\"entity\"]\n",
    "            if ent_type not in unique_entities:\n",
    "                continue\n",
    "            # print(output_entities)\n",
    "            if any(\n",
    "                [\n",
    "                    ent[\"text\"].replace(\" \", \"\")\n",
    "                    == ground_truth_ent[\"text\"].replace(\" \", \"\")\n",
    "                    and ent[\"start\"] == ground_truth_ent[\"start\"]\n",
    "                    for ent in output_entities\n",
    "                ]\n",
    "            ):\n",
    "                tp[ent_type] += 1\n",
    "            else:\n",
    "                fn[ent_type] += 1\n",
    "        for output_ent in output_entities:\n",
    "            ent_type = output_ent[\"entity\"]\n",
    "            if ent_type not in unique_entities:\n",
    "                continue\n",
    "            if output_ent[\"text\"].replace(\" \", \"\") not in ground_truth_texts:\n",
    "                fp[ent_type] += 1\n",
    "    # Calculate precision, recall, and F1 score for each entity\n",
    "    scores = {}\n",
    "    table = []\n",
    "    headers = [\"Entity\", \"Precision\", \"Recall\", \"F1\"]\n",
    "    for entity in unique_entities:\n",
    "        p = (\n",
    "            round(tp[entity] / (tp[entity] + fp[entity]), 2)\n",
    "            if tp[entity] + fp[entity] > 0\n",
    "            else 0\n",
    "        )\n",
    "        r = (\n",
    "            round(tp[entity] / (tp[entity] + fn[entity]), 2)\n",
    "            if tp[entity] + fn[entity] > 0\n",
    "            else 0\n",
    "        )\n",
    "        f1 = round(2 * p * r / (p + r), 2) if p + r > 0 else 0\n",
    "        scores[entity] = {\"precision\": p, \"recall\": r, \"f1\": f1}\n",
    "        table.append([entity, p, r, f1])\n",
    "\n",
    "    general_precision, general_recall, general_f1 = calculate_general_scores(scores)\n",
    "    report = f\"\\nGeneral Precision: {general_precision}\\nGeneral Recall: {general_recall}\\nGeneral F1: {general_f1}\\n{tabulate(table, headers)}\"\n",
    "    with open(output_file, \"w\", encoding=\"ISO-8859-1\") as f:\n",
    "        f.write(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_precision_recall_for_entities(\n",
    "    ground_truth,\n",
    "    prediction_data,\n",
    "    set(\n",
    "        [\"PERSON\", \"EMAIL_ADDRESS\", \"LOCATION\", \"US_SSN\", \"CREDIT_CARD\", \"PHONE_NUMBER\"]\n",
    "    ),\n",
    "    \"eval/REPORT_LG2.txt\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE CASE: BINARY\n",
    "\n",
    "- Given an input text, it indicates whether each entity (person, location, credit_card, phone_number, us_ssn) is present at least once or not.\n",
    "- The use case is applicable for filtering out reviews with sensitive information without needing to know which part of the text has the sensitive information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_entities(ground_truth, output_model, unique_entities, output_file):\n",
    "    # Initialize counters for true positives, false positives, and false negatives\n",
    "    tp = {entity: 0 for entity in unique_entities}\n",
    "    fp = {entity: 0 for entity in unique_entities}\n",
    "    fn = {entity: 0 for entity in unique_entities}\n",
    "\n",
    "    # Loop over the documents in the output model\n",
    "    for i, doc in enumerate(output_model):\n",
    "        # Get the set of entities present in the document\n",
    "        model_entity_set = set()\n",
    "        for entity in doc[\"ENTITIES\"]:\n",
    "            model_entity_set.add(entity[\"entity\"])\n",
    "        # Get the set of entities present in the ground truth in the same index of output model\n",
    "        ground_truth_set = set()\n",
    "        for entity in ground_truth[i][\"ENTITIES\"]:\n",
    "            ground_truth_set.add(entity[\"entity\"])\n",
    "\n",
    "        # Check if each entity in the ground truth is present in the output model\n",
    "        for entity in ground_truth_set:\n",
    "            if entity in model_entity_set:\n",
    "                # Entity is present in both ground truth and output model\n",
    "                tp[entity] += 1\n",
    "            else:\n",
    "                # Entity is present in ground truth but not in output model\n",
    "                fn[entity] += 1\n",
    "\n",
    "        # Check if each entity in the output model is a false positive\n",
    "        for entity in model_entity_set:\n",
    "            if entity not in ground_truth_set:\n",
    "                # Entity is not in the ground truth\n",
    "                fp[entity] += 1\n",
    "\n",
    "    # Calculate precision, recall, and F1 score for each entity\n",
    "    scores = {}\n",
    "    table = []\n",
    "    headers = [\"Entity\", \"Precision\", \"Recall\", \"F1\"]\n",
    "    for entity in unique_entities:\n",
    "        p = (\n",
    "            round(tp[entity] / (tp[entity] + fp[entity]), 2)\n",
    "            if tp[entity] + fp[entity] > 0\n",
    "            else 0\n",
    "        )\n",
    "        r = (\n",
    "            round(tp[entity] / (tp[entity] + fn[entity]), 2)\n",
    "            if tp[entity] + fn[entity] > 0\n",
    "            else 0\n",
    "        )\n",
    "        f1 = round(2 * p * r / (p + r), 2) if p + r > 0 else 0\n",
    "        scores[entity] = {\"precision\": p, \"recall\": r, \"f1\": f1}\n",
    "        table.append([entity, p, r, f1])\n",
    "\n",
    "    general_precision, general_recall, general_f1 = calculate_general_scores(scores)\n",
    "    report = f\"\\nGeneral Precision: {general_precision}\\nGeneral Recall: {general_recall}\\nGeneral F1: {general_f1}\\n{tabulate(table, headers)}\"\n",
    "    with open(output_file, \"w\", encoding=\"ISO-8859-1\") as f:\n",
    "        f.write(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1 = [{'TEXT': 'a1', \"ENTITIES\": [{'start': 21, 'end': 33, 'entity': 'PERSON', 'text': 'Zoey Edwards'}, {'start': 129, 'end': 151, 'entity': 'EMAIL_ADDRESS', 'text': 'edwards-zoey@gmail.com'}, {'start': 190, 'end': 201, 'entity': 'LOCATION', 'text': '900 F St NW'}, {'start': 213, 'end': 222, 'entity': 'US_SSN', 'text': '367245504'}, {'start': 252, 'end': 271, 'entity': 'CREDIT_CARD', 'text': '2259-8740-7030-1462'}]}]\n",
    "# a2 = [{\"TEXT\": \"a2\", \"ENTITIES\": [{'start': 21, 'end': 33, 'entity': 'PERSON', 'text': 'Zoey Edwards'}, {'start': 129, 'end': 151, 'entity': 'EMAIL_ADDRESS', 'text': 'edwards-zoey@gmail.com'}, {'start': 190, 'end': 195, 'entity': 'LOCATION', 'text': '900 F'}, {'start': 196, 'end': 201, 'entity': 'LOCATION', 'text': 'St NW'}, {'start': 213, 'end': 222, 'entity': 'US_SSN', 'text': '367245504'}, {'start': 252, 'end': 255, 'entity': 'PHONE_NUMBER', 'text': '225'}, {'start': 255, 'end': 271, 'entity': 'PHONE_NUMBER', 'text': '9-8740-7030-1462'}]}]\n",
    "evaluate_entities(\n",
    "    ground_truth,\n",
    "    prediction_data,\n",
    "    set(\n",
    "        [\"PERSON\", \"EMAIL_ADDRESS\", \"LOCATION\", \"US_SSN\", \"CREDIT_CARD\", \"PHONE_NUMBER\"]\n",
    "    ),\n",
    "    \"eval/REPORT_SENTENCE_LG.txt\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a9f895cc7e3ca0d623982e8f7234235464dc9c0b943e7d04631c8d05e4aca2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
